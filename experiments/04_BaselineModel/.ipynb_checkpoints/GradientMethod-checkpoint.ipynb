{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceee61af",
   "metadata": {},
   "source": [
    "This is based on Fabian Pedregosas [blog](https://fa.bianp.net/blog/2013/logistic-ordinal-regression/) and deprecated [github](https://github.com/fabianp/minirank/blob/master/minirank/logistic.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c954aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import optimize, linalg, sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e3cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ead39c",
   "metadata": {},
   "source": [
    "# Simulation Data\n",
    "\n",
    "Use a subsample of the simulation data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238cdea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from masterthesis.data import load_h5ad, load_acinar\n",
    "data = load_h5ad(\"/home/julian/Uni/MasterThesis/data/simdata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3914f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.var['Setting'] == \"TS\"  # extract only the time series samples\n",
    "sim_X = data.X[:, idx]\n",
    "sim_y = data.obs[\"Ordinal_Time_Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random subsample genes\n",
    "y_idx = rng.choice(np.arange(sim_y.size), size=sim_y.size // 2, replace=False)\n",
    "x_idx = rng.choice(np.arange(sim_X.shape[1]), size=sim_X.shape[1] // 4, replace=False)\n",
    "sim_X = sim_X[y_idx, :]\n",
    "sim_X = sim_X[:, x_idx]\n",
    "sim_y = sim_y[y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1769690",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "sim_X = scaler.fit_transform(sim_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ecaea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 1031)\n",
      "(286,)\n"
     ]
    }
   ],
   "source": [
    "print(sim_X.shape)\n",
    "print(sim_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a619471",
   "metadata": {},
   "source": [
    "# Acinar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02cfc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acinar_ann = load_acinar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3204e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected Genes after preprocessing in R\n",
    "sel_genes = [\"REG3A\", \"AMY2A\", \"MT2A\", \"OLFM4\",\n",
    "             \"SYCN\", \"CELA2B\", \"FGL1\", \"AMY2B\",\n",
    "             \"MT1G\", \"TM4SF1\", \"CELA2A\", \"PDK4\", \n",
    "             \"TACSTD2\", \"CD44\", \"PNLIPRP2\", \"ALB\", \n",
    "             \"ERP27\", \"LDHA\", \"REG3G\", \"CTRL\", \"CLPS\",\n",
    "             \"FOS\", \"HSPA8\", \"SERPINA3\", \"CELA3B\", \"CRP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37beff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ac_y = np.array([int(x) for x in acinar_ann.obs.donor_age])\n",
    "ac_label_conv = dict(zip(np.unique(ac_y), range(len(ac_y))))\n",
    "ac_y = np.array([ac_label_conv[l] for l in ac_y])\n",
    "k = len(np.unique(ac_y))\n",
    "\n",
    "ac_X_train, ac_X_test, ac_y_train, ac_y_test = train_test_split(acinar_ann[:,sel_genes].X, ac_y, \n",
    "                                                                test_size=0.1, \n",
    "                                                                stratify=ac_y,\n",
    "                                                                random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdcb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ac_X_train = scaler.fit_transform(ac_X_train)\n",
    "ac_X_test = scaler.fit_transform(ac_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382c85d",
   "metadata": {},
   "source": [
    "## Gradient DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1110e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG = 1e10\n",
    "SMALL = 1e-6\n",
    "\n",
    "def phi(beta, thresholds, X):\n",
    "    phi = 1 / (1 + np.exp(beta.T @ X - thresholds))\n",
    "    return phi\n",
    "\n",
    "def l1_regularizer(beta, l):\n",
    "    return l * np.sum(np.abs(beta))\n",
    "\n",
    "# PROBLEM: Is the objective minimized or maximized?? -> Scipy MINIMIZES\n",
    "# PROBLEM: What if diff is negative? -> Log has issues, but value may be meaningful\n",
    "# PROBLEM 2: What if diff is too small? -> Log will explode\n",
    "# PROBLEM 3: Loss of precision?\n",
    "# log likelihood with regularization as objective function\n",
    "def objective(params, X, y, k, lamb=0.1, scale_by_y=True):\n",
    "    \n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = np.array([theta[i] for i in y])\n",
    "    thresholds_m1 = np.array([theta[max(0, i-1)] for i in y]) \n",
    "\n",
    "    # fit term\n",
    "    idx = (thresholds > 0)\n",
    "    diff = np.zeros_like(thresholds)\n",
    "    phi_i = phi(beta, thresholds, X)\n",
    "    phi_im1 = phi(beta, thresholds_m1, X)\n",
    "    \n",
    "    diff[idx] = (phi_i - phi_im1)[idx]\n",
    "    diff[~idx] = phi_i[~idx]\n",
    "\n",
    "    # cut off the difference at a minimum to avoid PROBLEM 2\n",
    "    #if diff.min() < SMALL:\n",
    "    #    diff = np.maximum(diff, SMALL)\n",
    "    \n",
    "    loss = np.log(diff, out=np.zeros_like(diff), where=diff>0)\n",
    "    \n",
    "    loss = np.sum(loss)\n",
    "\n",
    "    # regularization term\n",
    "    loss += l1_regularizer(beta, lamb)\n",
    "    \n",
    "    # scale the loss with the inverse number of samples to handle PROBLEM 3\n",
    "    if scale_by_y:\n",
    "        loss *= (1 / y.size)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def objective_grad(params, X, y, k, lamb=0.1, scale_by_y=True):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = np.array([theta[i] for i in y])\n",
    "    thresholds_m1 = np.array([theta[max(0, i-1)] for i in y])\n",
    "\n",
    "    # PROBLEM: thresholds_m1 - thresholds can be 0 -> then the denominator becomes 0!\n",
    "    idx = (thresholds > 0)\n",
    "    diff = np.zeros_like(thresholds)\n",
    "    phi_i = phi(beta, thresholds, X)\n",
    "    phi_im1 = phi(beta, thresholds_m1, X)\n",
    "    \n",
    "    diff[idx] = (phi_i - phi_im1)[idx]\n",
    "    diff[~idx] = phi_i[~idx]\n",
    "    \n",
    "    # BETA UPDATE\n",
    "    beta_grad = np.sum(X * (1 - diff), axis=1)\n",
    "    \n",
    "    # derivative of regularizer over beta scaled\n",
    "    reg_sc = 1 / beta.size\n",
    "    beta_grad += np.sum(np.abs(beta)) * lamb * reg_sc\n",
    "        \n",
    "    # THETA UPDATE\n",
    "    # first half of the gradient\n",
    "    e = np.identity(k)\n",
    "    e_expanded = np.concatenate([e[i] for i in y]).reshape(y.size, k).T\n",
    "    \n",
    "    temp = (1 \n",
    "            - phi(beta, thresholds, X) \n",
    "            - np.exp(np.log(1 - (1 - np.exp(diff)))))\n",
    "    t1 = e_expanded @ temp\n",
    "    \n",
    "    # second half of the gradient\n",
    "    e_m1 = np.identity(k+1)[1:,:-1]  # identity with diagonal shifty up by one\n",
    "    e_m1_expanded = np.concatenate([e_m1[i] for i in y]).reshape(y.size, k).T\n",
    "    temp = (1 \n",
    "            - phi(beta, thresholds_m1, X) \n",
    "            - np.exp(np.log(1 - (1 - np.exp(-1 * (diff))))))\n",
    "    \n",
    "    t2 = e_m1_expanded @ temp\n",
    "    \n",
    "    theta_grad = t1 + t2\n",
    "    \n",
    "    # scaling\n",
    "    if scale_by_y:\n",
    "        theta_grad = theta_grad * (1/y.size)\n",
    "        beta_grad = beta_grad * (1/y.size)\n",
    "\n",
    "    return np.concatenate([beta_grad, theta_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b77b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def train(obj, grad, X, y, lamb=0.9, scaling=False, method=\"BFGS\"):\n",
    "    # flip such that X -> (genes, cells)\n",
    "    if X.shape[0] == y.size:\n",
    "        X = X.T\n",
    "    \n",
    "    n_classes = np.unique(y).size\n",
    "    params = np.zeros(X.shape[0] + n_classes)\n",
    "    \n",
    "    m = minimize(obj, params_nm, args=(X, y, n_classes, lamb, scaling), jac=grad, method=method)\n",
    "    print(m)\n",
    "    print(m.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b0ec987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Desired error not necessarily achieved due to precision loss.\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: -2.845559588887395\n",
      "        x: [ 1.180e-01  2.398e-02 ...  6.945e-02  1.042e-01]\n",
      "      nit: 1\n",
      "      jac: [ 7.387e-05 -5.406e-03 ... -1.175e-01 -4.962e-02]\n",
      " hess_inv: [[ 1.007e+00  4.210e-03 ...  6.452e-02  3.172e-02]\n",
      "            [ 4.210e-03  1.001e+00 ...  1.475e-02  8.905e-03]\n",
      "            ...\n",
      "            [ 6.452e-02  1.475e-02 ...  1.073e+00  7.200e-02]\n",
      "            [ 3.172e-02  8.905e-03 ...  7.200e-02  1.051e+00]]\n",
      "     nfev: 53\n",
      "     njev: 49\n",
      "[ 0.11802656  0.02397907 -0.06843315  0.08290751 -0.13818964 -0.12356077\n",
      "  0.09176323  0.03192918 -0.06979415  0.06221448 -0.10482473  0.03492728\n",
      "  0.04466238  0.12867758 -0.1010722   0.03643623 -0.07947574  0.06614485\n",
      " -0.01999933 -0.12323364 -0.11978792  0.01439815  0.0726311   0.12125875\n",
      " -0.12774478  0.0258153   0.06652065  0.1978226   0.17345736  0.25058858\n",
      "  0.04736112  0.21428774  0.06944616  0.10423809]\n"
     ]
    }
   ],
   "source": [
    "train(objective, objective_grad, ac_X_train, ac_y_train, lamb=0.1, scaling=True, method=\"bfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e416126",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Desired error not necessarily achieved due to precision loss.\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: -2286.831124850377\n",
      "        x: [ 4.881e-14 -2.820e-06 ...  2.298e-05  4.597e-05]\n",
      "      nit: 1\n",
      "      jac: [ 3.971e+01 -1.695e+01 ... -1.393e+06  1.392e+06]\n",
      " hess_inv: [[ 1.000e+00  1.123e-11 ... -9.156e-11 -1.831e-10]\n",
      "            [ 1.123e-11  9.997e-01 ...  2.741e-03  5.482e-03]\n",
      "            ...\n",
      "            [-9.156e-11  2.741e-03 ...  9.777e-01 -4.468e-02]\n",
      "            [-1.831e-10  5.482e-03 ... -4.468e-02  9.106e-01]]\n",
      "     nfev: 2142\n",
      "     njev: 61\n",
      "[ 4.88092707e-14 -2.81968583e-06 -2.98211487e-14  3.40207020e-14\n",
      " -2.81968590e-06 -2.81968589e-06 -2.81968580e-06 -5.63937166e-06\n",
      " -5.63937170e-06 -2.81968581e-06 -5.63937172e-06  1.37062171e-14\n",
      " -2.81968582e-06 -2.81968578e-06 -2.81968588e-06 -5.63937166e-06\n",
      "  1.19360628e-04  2.68464853e-14 -2.81968585e-06 -2.81968589e-06\n",
      " -2.81968588e-06 -5.63937167e-06 -5.63937164e-06 -5.63937162e-06\n",
      " -2.81968589e-06 -2.81968583e-06  1.25000000e-04 -5.63937167e-06\n",
      "  4.05396341e-05  9.62418884e-05  1.29279820e-05  9.76781957e-05\n",
      "  2.29832128e-05  4.59661989e-05]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "scale_by_y = False\n",
    "lamb = 0.9\n",
    "n_classes = np.unique(ac_y_train).size\n",
    "params = np.zeros(ac_X_train.shape[1] + n_classes)\n",
    "\n",
    "# random params\n",
    "params_rand = rng.normal(0, 0.1, ac_X_train.shape[1] + n_classes)\n",
    "\n",
    "# params after one run of Nelder-Mead \n",
    "params_nm = [2.99552789e-17, -9.12238797e-17,  8.43373503e-17,  6.28923016e-17,\n",
    "        1.29517064e-16,  2.35470728e-16,  4.64152560e-18, -2.37379072e-17,\n",
    "        9.33068062e-17, -1.31213310e-16,  8.47381150e-17, -9.14277033e-18,\n",
    "       -1.10647317e-16, -5.55542955e-17,  4.15674610e-17, -7.28795221e-17,\n",
    "        1.25000000e-04, -3.91915815e-17,  1.01399459e-16,  2.16536932e-16,\n",
    "        6.13107712e-15,  1.23078388e-16,  1.16260966e-16,  1.76600273e-17,\n",
    "        1.35598036e-16,  7.26746491e-17,  1.25000000e-04,  1.30296841e-16,\n",
    "        1.45441669e-16, -1.72678216e-17, -1.87164751e-16, -4.54465550e-17,\n",
    "       -1.00882237e-16, -3.29592381e-17]\n",
    "\n",
    "params_warm =  [0.47077975,  0.        ,  0.37275445,  0.        ,  0.        ,\n",
    "        -0.2718176 ,  0.01834662,  0.27956828,  0.        ,  0.00906079,\n",
    "         0.        ,  0.22041753, -0.1378634 ,  0.18726233, -0.08640451,\n",
    "         0.08268969, -0.26427331,  0.        , -0.28873376, -0.24587408,\n",
    "         0.        , -0.07656282, -0.02134246,  0.59662556,  0.05288458,\n",
    "         0.        ,  1.71402649,  0.10353958,  0.        , \n",
    "        -0.48783945, -0.62012817, -1.75287372, -2.09708938, 4]\n",
    "\n",
    "m = minimize(objective, params_nm, args=(ac_X_train.T, ac_y_train, n_classes, lamb, scale_by_y),\n",
    "         method=\"BFGS\")\n",
    "print(m)\n",
    "print(m.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c9eb8",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- Numerical instabilities (nans, infs): Especially BFGS struggles here and usually only executes one iteration\n",
    "- BFTS struggles to converge, starting from all-0 parameters.\n",
    "- Nelder-Mead works without derivative\n",
    "\n",
    "- None of the attempts introduce sparsity!\n",
    "- Using pretrained parameters (params_warm), all attempted solvers converge successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1fa5bff9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "Loss: 12345\n",
      "Weights: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iter 50\n",
      "Loss: -2093.3756390108347\n",
      "Weights: [-12.0354645    2.0764663    4.14784512  -5.36996877   9.10163742\n",
      "  11.27836069  -6.31736415  -2.25134098   5.06562685   3.87631968\n",
      "   6.5435978   -3.40019983   8.04123862  -2.23485703  10.42959417\n",
      "  -0.44177098   1.61206272  -1.54336137  -8.43191276   8.46552525\n",
      "   5.12071742  -0.57297819  -4.89048261  -2.91766763   8.16922764\n",
      "  -1.55641335  19.00705742  76.45716448  -3.20447185  50.96407881\n",
      "  36.05508629  12.12291515  30.05583119  21.12252796]\n"
     ]
    }
   ],
   "source": [
    "# SGD and our object function\n",
    "\n",
    "# training hyperparams\n",
    "eps = 0.0001\n",
    "dloss = 1\n",
    "cur_iter = 0\n",
    "max_iter = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# objective / grad hyperparams\n",
    "scaling = False\n",
    "lamb = 0.1\n",
    "\n",
    "# data \n",
    "X = ac_X_train.T\n",
    "y = ac_y_train\n",
    "\n",
    "# weights and data params\n",
    "n_classes = np.unique(y).size\n",
    "w = np.zeros(X.shape[0] + n_classes)\n",
    "\n",
    "losses = [12345]\n",
    "while (dloss > eps) and (cur_iter <= max_iter):\n",
    "    cur_iter += 1\n",
    "\n",
    "    if cur_iter % 50 == 0:\n",
    "        print(\"Iter\", cur_iter)\n",
    "        print(\"Loss:\",losses[-1])\n",
    "        print(\"Weights:\", w)\n",
    "        \n",
    "    losses.append(objective(w, X, y, n_classes, lamb, scale_by_y))\n",
    "    grad = objective_grad(w, X, y, n_classes, lamb, scale_by_y)\n",
    "    \n",
    "    w -= learning_rate * grad\n",
    "    \n",
    "    dloss = np.abs(losses[-2] - losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3287f0",
   "metadata": {},
   "source": [
    "## Use Jax autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d7b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit, vmap\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05155574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "LMBD = 0.5\n",
    "BIG = 1e10\n",
    "X = ac_X_train\n",
    "y = ac_y_train\n",
    "k = jnp.unique(ac_y_train).size\n",
    "\n",
    "def jphi(beta, thresholds):\n",
    "    phi = 1 / (1 + jnp.exp(jnp.dot(beta.T, X) - thresholds))\n",
    "    return phi\n",
    "\n",
    "def jl1_regularizer(beta):\n",
    "    return LMBD * jnp.sum(jnp.abs(beta))\n",
    "\n",
    "# PROBLEM: Is the objective minimized or maximized?? -> Scipy MINIMIZES\n",
    "# log likelihood with regularization as objective function\n",
    "def jobjective(params):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = jnp.array([theta[i] for i in y])\n",
    "    thresholds_m1 = jnp.array([theta[max(0, i-1)] for i in y]) \n",
    "\n",
    "    # fit term\n",
    "    # PROBLEM: What if diff is negative? -> Log has issues, but value may be meaningful\n",
    "    idx = (thresholds != 0)\n",
    "    diff = jnp.zeros_like(thresholds)\n",
    "    phi_i = jphi(beta, thresholds)\n",
    "    phi_im1 = jphi(beta, thresholds_m1)\n",
    "    diff.at[idx].set((phi_i - phi_im1)[idx])\n",
    "    diff.at[~idx].set(phi_i[~idx])\n",
    "\n",
    "    loss = jnp.log(diff)\n",
    "    loss = jnp.nan_to_num(jnp.log(diff), nan=0)\n",
    "    loss = jnp.sum(loss)\n",
    "\n",
    "    # regularization term\n",
    "    loss -= jl1_regularizer(beta)\n",
    "    \n",
    "    # scipy MINIMIZES the loss\n",
    "    loss *= -1\n",
    "\n",
    "    #print(\"avg loss:\", loss.mean())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e333efad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/share/virtualenvs/code-tW9RC7Ez/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:141: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n",
      "/home/julian/.local/share/virtualenvs/code-tW9RC7Ez/lib/python3.10/site-packages/scipy/optimize/_linesearch.py:382: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  alpha1 = min(1.0, 1.01*2*(phi0 - old_phi0)/derphi0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  message: Desired error not necessarily achieved due to precision loss.\n",
       "  success: False\n",
       "   status: 2\n",
       "      fun: inf\n",
       "        x: [ 0.000e+00  0.000e+00 ...  0.000e+00  0.000e+00]\n",
       "      nit: 0\n",
       "      jac: [ 1.000e-02  1.000e-02 ...  0.000e+00  0.000e+00]\n",
       " hess_inv: [[1 0 ... 0 0]\n",
       "            [0 1 ... 0 0]\n",
       "            ...\n",
       "            [0 0 ... 1 0]\n",
       "            [0 0 ... 0 1]]\n",
       "     nfev: 18\n",
       "     njev: 7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "LMBD = 0.01\n",
    "BIG = 1e10\n",
    "X = ac_X_train.T\n",
    "y = ac_y_train\n",
    "k = jnp.unique(ac_y_train).size\n",
    "params = np.zeros(ac_X_train.shape[1] + k)\n",
    "\n",
    "params_ones = np.ones(ac_X_train.shape[1] + k)\n",
    "\n",
    "params_warm_start = [0.47077975,  0.        ,  0.37275445,  0.        ,  0.        ,\n",
    "        -0.2718176 ,  0.01834662,  0.27956828,  0.        ,  0.00906079,\n",
    "         0.        ,  0.22041753, -0.1378634 ,  0.18726233, -0.08640451,\n",
    "         0.08268969, -0.26427331,  0.        , -0.28873376, -0.24587408,\n",
    "         0.        , -0.07656282, -0.02134246,  0.59662556,  0.05288458,\n",
    "         0.        ,  1.71402649,  0.10353958,  0.        , 0,  -0.48783945,\n",
    "        -0.62012817, -1.75287372, -2.09708938]\n",
    "\n",
    "\n",
    "\n",
    "objective_jaxder = grad(jobjective)\n",
    "minimize(jobjective, params, jac=objective_jaxder, method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a986a",
   "metadata": {},
   "source": [
    "## Gradient Model v3 \n",
    "\n",
    "Based on [mord by fabian](https://github.com/fabianp/mord/blob/master/mord/threshold_based.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67d818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "http://localhost:8888/?token=3783460b6309e1f75cfe99b263c1bf4fe50b5d7d8fd5bc66\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "def sigmoid(t):\n",
    "    # sigmoid function, 1 / (1 + exp(-t))\n",
    "    # stable computation\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss(Z):\n",
    "    # stable computation of the logistic loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = np.log(1 + np.exp(-Z[idx]))\n",
    "    out[~idx] = (-Z[~idx] + np.log(1 + np.exp(Z[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def obj_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * log_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    obj += alpha * (np.sum(np.abs(beta)))  # l1 term\n",
    "    return obj\n",
    "\n",
    "\n",
    "def grad_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Gradient for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "    # Alpha[idx] *= -1\n",
    "    # W[idx.T] *= -1\n",
    "\n",
    "    Sigma = S * loss_fd.T * sigmoid(-S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        Sigma *= sample_weight\n",
    "\n",
    "    grad_w = X.T.dot(Sigma.sum(0)) + alpha * w\n",
    "\n",
    "    grad_theta = -Sigma.sum(1)\n",
    "    grad_c = L.T.dot(grad_theta)\n",
    "    return np.concatenate((grad_w, grad_c), axis=0)\n",
    "\n",
    "\n",
    "def threshold_fit(X, y, alpha, n_class, mode='AE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'SE'}\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1))\n",
    "    elif mode == '0-1':\n",
    "        loss_fd = np.diag(np.ones(n_class - 1)) + \\\n",
    "            np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = 1  # border case\n",
    "    elif mode == 'SE':\n",
    "        a = np.arange(n_class-1)\n",
    "        b = np.arange(n_class)\n",
    "        loss_fd = np.abs((a - b[:, None])**2 - (a - b[:, None]+1)**2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "        jac=grad_margin, bounds=bounds, options=options,\n",
    "        args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "        tol=tol)\n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta\n",
    "\n",
    "threshold_fit(ac_X_train, ac_y_train, 1, np.unique(ac_y_train).size, mode=\"AE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
