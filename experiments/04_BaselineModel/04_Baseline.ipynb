{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42a1acc",
   "metadata": {},
   "source": [
    "# Baseline Model in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30bf9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import logit\n",
    "from sklearn.metrics import accuracy_score, log_loss as cross_entropy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d938a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=1234321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d993b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masterthesis.data import load_acinar, data_dir\n",
    "# load the python AnnData object\n",
    "acinar_ann = load_acinar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245f5c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape (411, 23368)\n",
      "First gene: (411,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shape\", acinar_ann.X.shape)\n",
    "print(\"First gene:\", acinar_ann.X[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53b44e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1/2-SBSRNA4', 'A1BG', 'A1BG-AS1', 'A1CF', 'A2LD1', 'A2M', 'A2ML1',\n",
       "       'A2MP1', 'A4GALT', 'A4GNT',\n",
       "       ...\n",
       "       'ZWINT', 'ZXDA', 'ZXDB', 'ZXDC', 'ZYG11A', 'ZYG11B', 'ZYX', 'ZZEF1',\n",
       "       'ZZZ3', 'tAKR'],\n",
       "      dtype='object', length=23368)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access Gene Names\n",
    "acinar_ann.var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cc30fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 411 × 1\n",
       "    obs: 'wellKey', 'geo_accession', 'donor_age', 'gender', 'inferred_cell_type'\n",
       "    var: 'primerid'\n",
       "    uns: 'X_name'\n",
       "    layers: 'counts', 'tpm'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select one gene\n",
    "acinar_ann[:, acinar_ann.var_names.str.match(\"A1CF\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7809d",
   "metadata": {},
   "source": [
    "### R Gene selection and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3fd1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling in R with seed 1234\n",
    "test_idx = [284, 336, 406, 101, 111, 393, 133, 400, 388, 98, 103, 214, 90, 326, 79, 372, 270, 382, 184, 62, 4, 403, 149, 40, 212, 195, 93, 122, 66, 175, 379, 304, 108, 131, 343, 41, 115, 228, 328, 298, 299]\n",
    "train_idx = list(set(range(acinar_ann.X.shape[0])) - set(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc58a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected Genes after preprocessing in R\n",
    "sel_genes = [\"REG3A\", \"AMY2A\", \"MT2A\", \"OLFM4\",\n",
    "             \"SYCN\", \"CELA2B\", \"FGL1\", \"AMY2B\",\n",
    "             \"MT1G\", \"TM4SF1\", \"CELA2A\", \"PDK4\", \n",
    "             \"TACSTD2\", \"CD44\", \"PNLIPRP2\", \"ALB\", \n",
    "             \"ERP27\", \"LDHA\", \"REG3G\", \"CTRL\", \"CLPS\",\n",
    "             \"FOS\", \"HSPA8\", \"SERPINA3\", \"CELA3B\", \"CRP\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e698b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = np.array([int(x) for x in acinar_ann.obs.donor_age])\n",
    "k = len(np.unique(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(acinar_ann[:,sel_genes].X, y, \n",
    "                                                    test_size=0.1, \n",
    "                                                    stratify=y,\n",
    "                                                    random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd408d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old approach based on the indexes from R\n",
    "#y_train = y[train_idx]\n",
    "#y_test = y[test_idx]\n",
    "\n",
    "#X_train = acinar_ann[test_idx, sel_genes].X\n",
    "#X_test = acinar_ann[train_idx, sel_genes].X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e11daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb3bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X: (42, 26)\n",
      "Test y: (42,)\n",
      "Train X: (369, 26)\n",
      "Train y: (369,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test X:\", X_test.shape)\n",
    "print(\"Test y:\", y_test.shape)\n",
    "print(\"Train X:\", X_train.shape)\n",
    "print(\"Train y:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788e371",
   "metadata": {},
   "source": [
    "## Model 1: mord\n",
    "\n",
    "**Result: It was not possible to achieve the necessary sparsity with this model. Only L2 regularization is required. The thresholds seem not entirely plausible**\n",
    "\n",
    "[Reference 1](https://medium.datadriveninvestor.com/logistic-regression-simple-multinomial-and-ordinal-b2bc886bb974) [Reference 2](https://pythonhosted.org/mord/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33cf900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mord in /home/julian/.local/share/virtualenvs/code-tW9RC7Ez/lib/python3.10/site-packages (0.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e66f6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mord import LogisticAT, LogisticIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "babd36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y into a series of subsequent labels [0,1,2 ...]\n",
    "transf = dict(zip(np.unique(y),\n",
    "                  np.arange(0, len(np.unique(y)))))\n",
    "                        \n",
    "y_train_trans = np.array([transf[e] for e in y_train])\n",
    "y_test_trans = np.array([transf[e] for e in y_test])\n",
    "\n",
    "# reordering, such that yi < yi+1\n",
    "train_reorder = np.argsort(y_train_trans)\n",
    "test_reorder = np.argsort(y_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69f7ff8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2619047619047619"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all-threshold fit\n",
    "regressor = LogisticAT(verbose=0, alpha=0.1)\n",
    "regressor.fit(X_train, y_train_trans)\n",
    "regressor.score(X_test, y_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b60334c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# immediate-threshold fit\n",
    "regressor = LogisticIT(verbose=0, alpha=0, )\n",
    "regressor.fit(X_train[train_reorder], y_train_trans[train_reorder])\n",
    "regressor.score(X_test[test_reorder], y_test_trans[test_reorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feef9d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44116335,  0.34111261,  0.46324913,  0.03337747,  0.49242948,\n",
       "       -0.38978002,  0.1064584 ,  0.30709031, -0.01977034,  0.15211454,\n",
       "       -0.31710565,  0.13906532, -0.08686401,  0.48321721, -0.11721669,\n",
       "        0.24753368, -0.30225062,  0.07715455, -0.21801211, -0.37108485,\n",
       "       -1.02519282, -0.2038906 , -0.1342585 ,  0.38484625,  0.97416737,\n",
       "       -0.01220314])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f65a05b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.3533254 ,  0.48773183,  0.48773183,  0.65012671,  0.65012671,\n",
       "        1.98465256,  1.98465256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.theta_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7e1b6",
   "metadata": {},
   "source": [
    "## Model 2: Ordered Multinomial Regression (statsmodels) \n",
    "\n",
    "**Result: Introducing sparsity, or even using any regularizer seems to not be supported, or at least I didn't find a way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c612834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8b490d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.342428\n",
      "         Iterations: 63\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OrderedModel Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>             <td>y</td>         <th>  Log-Likelihood:    </th> <td> -495.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>OrderedModel</td>    <th>  AIC:               </th> <td>   1057.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>   1186.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Fri, 28 Apr 2023</td>  <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>11:48:29</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>   369</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>   336</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>    33</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_prob = OrderedModel(y_train,\n",
    "                        X_train,\n",
    "                        distr='logit')\n",
    "\n",
    "res_prob = mod_prob.fit(method='bfgs')\n",
    "res_prob.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4816fe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.59097453e-01,  3.60263390e-01,  7.59210347e-01,  8.26093583e-02,\n",
       "        7.36748064e-01, -7.79475910e-01,  1.57168024e-01,  5.29276977e-01,\n",
       "       -2.81765249e-03,  2.37234152e-01, -2.63643935e-01,  3.54884561e-01,\n",
       "       -3.89196449e-01,  7.37195957e-01, -3.59803593e-01,  3.94366068e-01,\n",
       "       -5.52783934e-01,  7.23712845e-02, -3.52474961e-01, -6.70490399e-01,\n",
       "       -1.20057630e+00, -2.86066009e-01, -1.00226058e-01,  8.30967472e-01,\n",
       "        1.30308024e+00, -9.53359675e-03, -3.59336789e+00,  1.11975598e+00,\n",
       "       -3.74626758e-01,  3.04805248e-01, -1.70403323e+00,  6.37809153e-01,\n",
       "       -2.64526938e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_prob.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "438cde48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [3 1 5 3 0 7 1 3 1 1 1 1 0 5 5 1 3 1 3 5 7 1 5 3 3 3 1 0 5 5 1 3 0 1 5 1 0\n",
      " 5 0 5 1 5]\n",
      "Ground Truth: [5 2 3 1 4 7 5 2 1 6 3 3 0 5 7 1 1 5 5 6 7 1 3 1 1 7 1 0 5 3 1 2 0 2 3 5 0\n",
      " 5 0 1 1 3]\n",
      "Cross Entropy: 1.6736239415646632\n",
      "Accuracy: 0.38095238095238093\n"
     ]
    }
   ],
   "source": [
    "predictions = res_prob.model.predict(res_prob.params, exog=X_test)\n",
    "print(\"Predictions:\", predictions.argmax(1))\n",
    "print(\"Ground Truth:\", y_test_trans)\n",
    "print(\"Cross Entropy:\", cross_entropy_score(y_test, predictions, labels=np.unique(y)))\n",
    "print(\"Accuracy:\", accuracy_score(predictions.argmax(1), y_test_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c7457",
   "metadata": {},
   "source": [
    "## Model 3: Multinomial Regression (sklearn)\n",
    "\n",
    "**Results:** \n",
    "- Introduction of sparsity worked well with the l1 penalty\n",
    "- Prediction results were on par with the other methods\n",
    "- However, since this is solved as a multinomial regression problem, one set of parameters is fit for each prediction class. This introduces a new problem for selection of parameters: The weights have to be aggregated, which has eliminated the sparsity with the attempted approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83b26393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/share/virtualenvs/code-tW9RC7Ez/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, penalty=&#x27;l1&#x27;, random_state=12345,\n",
       "                   solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, penalty=&#x27;l1&#x27;, random_state=12345,\n",
       "                   solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', penalty='l1', random_state=12345,\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk_model = LogisticRegression(penalty=\"l1\",\n",
    "                              multi_class=\"multinomial\", # \"auto\", \"ovr\", \"multinomial\"\n",
    "                              solver=\"saga\",\n",
    "                              random_state=12345)\n",
    "sk_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344b7d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients shape: (8, 26)\n",
      "Train score: 0.9512195121951219\n",
      "Test score: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Model coefficients shape:\", sk_model.coef_.shape)\n",
    "print(\"Train score:\", sk_model.score(X_train, y_train))\n",
    "print(\"Test score:\", sk_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a038c",
   "metadata": {},
   "source": [
    "#### Aggregation of Weights from multinomial model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b7baf46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added weights from Muli-Class model\n",
      "sparsity: 0\n",
      "Average weights from Muli-Class model\n",
      "sparsity: 0\n"
     ]
    }
   ],
   "source": [
    "# defines a threshold below which a gene is not considered significant\n",
    "# this is arbitrary, there is no way of defining this\n",
    "sparsity_threshold = 0.0001\n",
    "\n",
    "skl_mm_added = np.add.reduce(sk_model.coef_, axis=0)\n",
    "print(\"Added weights from Muli-Class model\")\n",
    "print(\"sparsity:\", sum(np.abs(skl_mm_added) < sparsity_threshold))\n",
    "\n",
    "skl_mm_mean = skl_mm_added / sk_model.coef_.shape[1]\n",
    "print(\"Average weights from Muli-Class model\")\n",
    "print(\"sparsity:\", sum(np.abs(skl_mm_mean) < sparsity_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528d755",
   "metadata": {},
   "source": [
    "## Models 4-6: Binary LogisticRegression (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ead1a",
   "metadata": {},
   "source": [
    "### Convert the data\n",
    "\n",
    "- The labels are converted to binary, such that the threshold from 0-1 corresponds from changing from label $l_i$ to $l_{i+1}$. $k$ copies of the label vector are concatenated such that for every vector $j$ the labels  $l_i$ with $i<j$ are converted to 0 and the labels $i\\ge j$ are converted to 1.\n",
    "- The count matrix is extended with copies of itself, to fit the converted label vector FOR NOW. For big problems, it could suffice to have just one label vector and perform and iterative training.\n",
    "- To train the thresholds, $k$ columns are added to the count matrix and initialized to zero. Each column column represents the threshold for a label $l_i$ and is set to 1, exactly  where that label $l_1$ occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa5f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bin_y(y_orig):\n",
    "    y_classes = np.unique(y_orig)\n",
    "    k = len(y_classes)\n",
    "\n",
    "    y_bin = []\n",
    "    for ki in range(1,k):\n",
    "        thresh = y_classes[ki]\n",
    "        y_bin += [int(x>=thresh) for x in y_orig]\n",
    "\n",
    "    y_bin = np.array(y_bin)\n",
    "    \n",
    "    return y_bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "094cbf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bin_X(X_orig, k):\n",
    "\n",
    "    # X training matrix\n",
    "    X_bin = np.concatenate([X_orig.copy()] * (k-1))\n",
    "    # Add thresholds\n",
    "    num_el = X_orig.shape[0] * (k-1)\n",
    "\n",
    "    for ki in range(k-1):\n",
    "        temp = np.repeat(0, num_el).reshape(X_orig.shape[0], (k-1))\n",
    "        temp[:,ki] = 1\n",
    "        if ki > 0:\n",
    "            thresholds = np.concatenate([thresholds, temp])\n",
    "        else:\n",
    "            thresholds = temp\n",
    "\n",
    "    X_bin = np.concatenate([X_bin, thresholds], axis=1)\n",
    "\n",
    "    return X_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc2e104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: 2583\n",
      "y_test: 294\n"
     ]
    }
   ],
   "source": [
    "y_train_bin = to_bin_y(y_train)\n",
    "print(\"y_train:\", len(y_train_bin))\n",
    "\n",
    "y_test_bin = to_bin_y(y_test)\n",
    "print(\"y_test:\", len(y_test_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "986de3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_bin: (2583, 33)\n",
      "X_test_bin: (294, 33)\n"
     ]
    }
   ],
   "source": [
    "X_train_bin = to_bin_X(X_train, k=np.unique(y).size)\n",
    "print(\"X_train_bin:\", X_train_bin.shape)\n",
    "\n",
    "X_test_bin = to_bin_X(X_test, k=np.unique(y).size)\n",
    "print(\"X_test_bin:\", X_test_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2374c4aa",
   "metadata": {},
   "source": [
    "### Model 4: LogisticRegression (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "926af278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.01, fit_intercept=False, max_iter=10000, penalty=&#x27;l1&#x27;,\n",
       "                   random_state=1234, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.01, fit_intercept=False, max_iter=10000, penalty=&#x27;l1&#x27;,\n",
       "                   random_state=1234, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=0.01, fit_intercept=False, max_iter=10000, penalty='l1',\n",
       "                   random_state=1234, solver='liblinear')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk_binlogreg_model = LogisticRegression(penalty=\"l1\", \n",
    "                                  fit_intercept=False,\n",
    "                                  max_iter=10000,\n",
    "                                  solver=\"liblinear\",\n",
    "                                  random_state=1234,\n",
    "                                  C=0.01  # Inverse of regularization strength -> controls sparsity in our case!\n",
    "                                 )\n",
    "\n",
    "sk_binlogreg_model.fit(X_train_bin, y_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c94cb891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7770034843205574\n",
      "Test score: 0.7653061224489796\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score:\",sk_binlogreg_model.score(X_train_bin, y_train_bin))\n",
    "print(\"Test score:\",sk_binlogreg_model.score(X_test_bin, y_test_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "136e2408",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27050863,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        -0.01287085,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.07369993,  0.        ,  0.10880254,  0.        ,\n",
       "         0.        , -0.13150305,  0.        , -0.10582882, -0.0518537 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.35514256,  0.        ,\n",
       "         0.        ,  0.4170856 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.44601961, -0.64557786]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_binlogreg_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea711fa",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression with GLMnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "635bc330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glmnet\n",
      "  Using cached glmnet-2.2.1.tar.gz (90 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[14 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-120n7fwd/glmnet_9643fca68e8540889f6e25a520cd506f/setup.py:14: DeprecationWarning:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "  \u001b[31m   \u001b[0m   of the deprecation of `distutils` itself. It will be removed for\n",
      "  \u001b[31m   \u001b[0m   Python >= 3.12. For older Python versions it will remain present.\n",
      "  \u001b[31m   \u001b[0m   It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "  \u001b[31m   \u001b[0m   For more details, see:\n",
      "  \u001b[31m   \u001b[0m     https://numpy.org/devdocs/reference/distutils_status_migration.html\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   from numpy.distutils.core import Extension, setup\n",
      "  \u001b[31m   \u001b[0m /home/julian/.local/share/virtualenvs/code-tW9RC7Ez/lib/python3.10/site-packages/setuptools/__init__.py:85: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated. Requirements should be satisfied by a PEP 517 installer. If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m error in glmnet setup command: 'python_requires' must be a string containing valid version specifiers; Invalid specifier: '>=3.6.*'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1dffa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement glment-py (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for glment-py\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install glment-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8cca1",
   "metadata": {},
   "source": [
    "* The originally used package `glment` no longer works\n",
    "* glmnet_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b5882a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glmnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mglmnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogitNet\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Note: Alpha is the regularization mixing parameter: alpha=1 -> L1, alpha=0 -> L2, 0<alpha<1 -> elastic net \u001b[39;00m\n\u001b[1;32m      4\u001b[0m glmnet_model \u001b[38;5;241m=\u001b[39m LogitNet(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m                         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                         standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# already standardized\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m,\n\u001b[1;32m      8\u001b[0m                         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glmnet'"
     ]
    }
   ],
   "source": [
    "from glmnet import LogitNet\n",
    "\n",
    "# Note: Alpha is the regularization mixing parameter: alpha=1 -> L1, alpha=0 -> L2, 0<alpha<1 -> elastic net \n",
    "glmnet_model = LogitNet(alpha=1,\n",
    "                        fit_intercept=False,\n",
    "                        standardize=False, # already standardized\n",
    "                        random_state=1234,\n",
    "                        max_iter=10000)\n",
    "glmnet_model.fit(X_train_bin, y_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90143035",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Train score: \", glmnet_model.score(X_train_bin, y_train_bin))\n",
    "print(\"Test score: \", glmnet_model.score(X_test_bin, y_test_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41071ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glmnet_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c88de7",
   "metadata": {},
   "source": [
    "## SGD with mini Batches\n",
    "\n",
    "To reduce the memory load, this introduces a sampling method with an iterative training paradigm\n",
    "\n",
    "ToDo: \n",
    "\n",
    "    - Construct matrix on the fly\n",
    "    - Check convergence / early stopping\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71ffaa91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:  2 Train score:  0.871517027863777\n",
      "Iter:  4 Train score:  0.8769349845201239\n",
      "Iter:  6 Train score:  0.8769349845201239\n",
      "Iter:  8 Train score:  0.8630030959752322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier(loss=\"log_loss\",\n",
    "                          penalty=\"l1\",\n",
    "                          alpha=0.01,  # = lambda in paper!! Very important to tune for the desired sparsity!\n",
    "                          fit_intercept=False,\n",
    "                          n_jobs=1)\n",
    "\n",
    "cur_iter = 0\n",
    "max_iter = 10\n",
    "n_batches = 2\n",
    "\n",
    "while cur_iter < max_iter:\n",
    "    if (cur_iter > 0 and cur_iter % 2 == 0):\n",
    "        print(\"Iter: \", cur_iter, \"Train score: \", sgd_model.score(X_batch, y_batch))\n",
    "    \n",
    "    cur_iter += 1\n",
    "    \n",
    "    # fit from samples of the big matrix\n",
    "    # TODO: Sampling from the big matrix directly is just for PoP,\n",
    "    # and eliminates the purpose. Only the binarized y-vector should\n",
    "    # be created and the indexes taken from the log count matrix.\n",
    "    sampled_indices = np.random.randint(X_train_bin.shape[0], size=X_train_bin.shape[0])\n",
    "\n",
    "    start = 0\n",
    "    for i in range(1, n_batches+1):\n",
    "        end = (i * X_train_bin.shape[0] // n_batches)\n",
    "        idx = sampled_indices[start:end]\n",
    "        X_batch = X_train_bin[idx,:]\n",
    "        y_batch = y_train_bin[idx]\n",
    "        start = end\n",
    "        sgd_model.partial_fit(X_batch, y_batch, classes=np.unique(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "249cd2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8563685636856369\n",
      "Test score: 0.8129251700680272\n"
     ]
    }
   ],
   "source": [
    "print(\"Train score:\", sgd_model.score(X_train_bin, y_train_bin))\n",
    "print(\"Test score:\", sgd_model.score(X_test_bin, y_test_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05f06585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46454191,  0.        ,  0.2845817 ,  0.        ,  0.        ,\n",
       "        -0.28953277,  0.01554047,  0.28022325,  0.        ,  0.10291084,\n",
       "         0.        ,  0.25152763, -0.17590685,  0.21141084, -0.17147189,\n",
       "         0.07721453, -0.30291864,  0.        , -0.29268156, -0.274818  ,\n",
       "        -0.00498819, -0.08100443, -0.09887985,  0.72301699,  0.08763479,\n",
       "         0.        ,  1.83388177,  0.1697285 ,  0.        , -0.55948728,\n",
       "        -0.69876035, -1.88989166, -2.31233195]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b5cff",
   "metadata": {},
   "source": [
    "# Compare Parameters\n",
    "\n",
    "- To compare parameters we first fincd the best regularization strength\n",
    "    * The best regularization has the highest score across 5-fold CV\n",
    "    * To increase sparsity, we choose the parameter with highest regularization, that lies within 1 standard error of the optimum\n",
    "- Then we do N fits with different seeds and collect the parameter values. \n",
    "- Finally we compare the distributions of the collected parameter values visually and wrt KL-divergence\n",
    "\n",
    "The models to investigate:  sklearn LinRegressor, GLMnet Mmodel, SGD LinRegressor, and Psupertime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dff745b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV\n",
    "\n",
    "n_folds = 5\n",
    "kf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "# elongate the origial non-binarized y-train data\n",
    "# to enable stratification\n",
    "y_train_elong = np.repeat(y_train, k-1)\n",
    "\n",
    "cv_splits = kf.split(X_train_bin, y_train_elong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88520616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
    "\n",
    "scorers = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"cross-entropy\": log_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "046e86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dof(params):\n",
    "    return np.count_nonzero(params != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70e7fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_res_to_df(cv_results, scorers, reg_params=None):\n",
    "    df = pd.DataFrame.from_dict(cv_results, orient=\"index\").stack().to_frame()\n",
    "    df = pd.DataFrame(df[0].values.tolist(), index=df.index)\n",
    "    \n",
    "    if reg_params is not None:\n",
    "        df.columns = [\"L=%s\" % x for x in reg_params]\n",
    "\n",
    "    for scorer in scorers.keys():\n",
    "        df.loc[(\"mean\", scorer), :] = df.xs(scorer, level=1).mean(axis=0)\n",
    "    df.loc[(\"mean\", \"dof\"), :] = df.xs(\"dof\",level=1).mean(axis=0)\n",
    "    \n",
    "    return df.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c649ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_param(res_df, reg_params, lower_increases_reg=True):\n",
    "    trimmed = res_df.loc[res_df[(\"mean\", \"dof\")] != 0]\n",
    "    trimmed_max = trimmed[(\"mean\", \"accuracy\")].max()\n",
    "    trimmed_std = trimmed[(\"mean\", \"accuracy\")].std()\n",
    "    thresh = trimmed_max - trimmed_std\n",
    "    above = trimmed[trimmed[(\"mean\", \"accuracy\")] > thresh]\n",
    "\n",
    "    if lower_increases_reg:\n",
    "        idx = above.iloc[-1].name\n",
    "    else:\n",
    "        idx = above.iloc[0].name\n",
    "\n",
    "    print(\"max:\", trimmed_max, \"std:\", trimmed_std, \"thresh:\", thresh)\n",
    "    print(\"Best average fit:\", trimmed.loc[idx])\n",
    "    print(\"Best parameter:\", reg_params[idx])\n",
    "    \n",
    "    return reg_params[idx]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5174e0f",
   "metadata": {},
   "source": [
    "**Important result: Choice of regularization path (lambda path) is critical for selection of best parameter!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30450388",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = np.concatenate((np.linspace(1,10,10)[::-1], np.logspace(1, 15, 40, base=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7435219",
   "metadata": {},
   "source": [
    "## SKLearn linregressor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRIDSEARCH:\n",
    "# ------------\n",
    "\n",
    "#scorers = {\n",
    "#    \"accuracy\": make_scorer(accuracy_score),\n",
    "#    \"cross-entropy-loss\": make_scorer(log_loss)\n",
    "#}\n",
    "#params = {\"C\": [1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]}\n",
    "#sk_binlogreg_model = LogisticRegression(penalty=\"l1\", \n",
    "#                                  fit_intercept=False,\n",
    "#                                  max_iter=10000,\n",
    "#                                  solver=\"liblinear\",\n",
    "#                                  random_state=1234)\n",
    "#sk_binlogreg_cv = GridSearchCV(sk_binlogreg_model,\n",
    "#                               param_grid=params,\n",
    "#                               refit=False,\n",
    "#                               cv=kf.split(X_train_bin, y_train_elong),\n",
    "#                               scoring=scorers)\n",
    "#sk_binlogreg_cv.cv_results_\n",
    "\n",
    "# Problem: Does not save parameters of intermediate models: Tracking of sparsity not possible\n",
    "# Let's do it ourselves ..\n",
    "# But, still interesting for the final package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f3336d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_0\n",
      "split_1\n",
      "split_2\n",
      "split_3\n",
      "split_4\n"
     ]
    }
   ],
   "source": [
    "#reg_params = [1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01, 0.005, 0.001, 0.0005]\n",
    "sk_reg_params = np.concatenate((np.linspace(1,5,10)[::-1], np.logspace(1, 15, 40, base=0.5)))\n",
    "cv_results = dict()\n",
    "\n",
    "for i, (cv_train_idx, cv_test_idx) in enumerate(kf.split(X_train_bin, y_train_elong)):\n",
    "    \n",
    "    s = \"split_%s\" % i\n",
    "    print(s)\n",
    "\n",
    "    cv_results[s] = dict()\n",
    "    cv_results[s][\"dof\"] = []\n",
    "    for scorer in scorers.keys():\n",
    "        cv_results[s][scorer] = []\n",
    "    \n",
    "    for c in sk_reg_params:\n",
    "        model = LogisticRegression(penalty=\"l1\",\n",
    "                                   C=c,\n",
    "                                   fit_intercept=False,\n",
    "                                   max_iter=10000,\n",
    "                                   solver=\"liblinear\",\n",
    "                                   random_state=1357);\n",
    "        \n",
    "        model.fit(X_train_bin[cv_train_idx,] , y_train_bin[cv_train_idx])\n",
    "        \n",
    "        for scorer in scorers.keys():\n",
    "            predicted = model.predict(X_train_bin[cv_test_idx,])            \n",
    "            score = scorers[scorer](y_train_bin[cv_test_idx], predicted)\n",
    "            cv_results[s][scorer].append(score)\n",
    "        \n",
    "        cv_results[s][\"dof\"].append(dof(model.coef_))\n",
    "\n",
    "\n",
    "sk_linreg_res = cv_res_to_df(cv_results, scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a137bec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_3</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_4</th>\n",
       "      <th colspan=\"3\" halign=\"left\">mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.837524</td>\n",
       "      <td>5.856222</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>8.435749</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.874031</td>\n",
       "      <td>4.540383</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.789761</td>\n",
       "      <td>7.577798</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.837524</td>\n",
       "      <td>5.856222</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>8.435749</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.874031</td>\n",
       "      <td>4.540383</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.789761</td>\n",
       "      <td>7.577798</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.837524</td>\n",
       "      <td>5.856222</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.767892</td>\n",
       "      <td>8.366032</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.874031</td>\n",
       "      <td>4.540383</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.790147</td>\n",
       "      <td>7.563855</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.839458</td>\n",
       "      <td>5.786505</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.767892</td>\n",
       "      <td>8.366032</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>4.610235</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.790147</td>\n",
       "      <td>7.563882</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.839458</td>\n",
       "      <td>5.786505</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.767892</td>\n",
       "      <td>8.366032</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>4.610235</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.790147</td>\n",
       "      <td>7.563882</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.837524</td>\n",
       "      <td>5.856222</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.767892</td>\n",
       "      <td>8.366032</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>4.610235</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.789760</td>\n",
       "      <td>7.577825</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.837524</td>\n",
       "      <td>5.856222</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.767892</td>\n",
       "      <td>8.366032</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>4.610235</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.790147</td>\n",
       "      <td>7.563882</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>6.135090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.769826</td>\n",
       "      <td>8.296315</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>4.610235</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.788986</td>\n",
       "      <td>7.605712</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.825919</td>\n",
       "      <td>6.274524</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.771760</td>\n",
       "      <td>8.226598</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>4.610235</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.788599</td>\n",
       "      <td>7.619655</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>6.413958</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.777563</td>\n",
       "      <td>8.017447</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.870155</td>\n",
       "      <td>4.680087</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.788599</td>\n",
       "      <td>7.619682</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.764023</td>\n",
       "      <td>8.505466</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.785300</td>\n",
       "      <td>7.738579</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.810445</td>\n",
       "      <td>6.832259</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.868217</td>\n",
       "      <td>4.749939</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.653101</td>\n",
       "      <td>12.503515</td>\n",
       "      <td>0.776217</td>\n",
       "      <td>8.065952</td>\n",
       "      <td>30.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.729207</td>\n",
       "      <td>9.760370</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.783366</td>\n",
       "      <td>7.808296</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.806576</td>\n",
       "      <td>6.971693</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.858527</td>\n",
       "      <td>5.099199</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.649225</td>\n",
       "      <td>12.643220</td>\n",
       "      <td>0.765380</td>\n",
       "      <td>8.456556</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.688588</td>\n",
       "      <td>11.224426</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.791103</td>\n",
       "      <td>7.529429</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.812379</td>\n",
       "      <td>6.762542</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>5.308755</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.649225</td>\n",
       "      <td>12.643220</td>\n",
       "      <td>0.758802</td>\n",
       "      <td>8.693674</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.644101</td>\n",
       "      <td>12.827915</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.793037</td>\n",
       "      <td>7.459712</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.806576</td>\n",
       "      <td>6.971693</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>5.588163</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>12.713072</td>\n",
       "      <td>0.747192</td>\n",
       "      <td>9.112111</td>\n",
       "      <td>29.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.644101</td>\n",
       "      <td>12.827915</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>7.599145</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.802708</td>\n",
       "      <td>7.111127</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.839147</td>\n",
       "      <td>5.797719</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>12.992480</td>\n",
       "      <td>0.742932</td>\n",
       "      <td>9.265677</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.651838</td>\n",
       "      <td>12.549048</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.804642</td>\n",
       "      <td>7.041410</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.796905</td>\n",
       "      <td>7.320278</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.831395</td>\n",
       "      <td>6.077128</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.744088</td>\n",
       "      <td>9.224009</td>\n",
       "      <td>28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.655706</td>\n",
       "      <td>12.409614</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.806576</td>\n",
       "      <td>6.971693</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.793037</td>\n",
       "      <td>7.459712</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.815891</td>\n",
       "      <td>6.635944</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.741374</td>\n",
       "      <td>9.321829</td>\n",
       "      <td>27.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.657640</td>\n",
       "      <td>12.339897</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.781431</td>\n",
       "      <td>7.878013</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.804264</td>\n",
       "      <td>7.055056</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.739048</td>\n",
       "      <td>9.405652</td>\n",
       "      <td>26.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>12.061029</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.823985</td>\n",
       "      <td>6.344241</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>8.644899</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.777132</td>\n",
       "      <td>8.032985</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.732461</td>\n",
       "      <td>9.643068</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.667311</td>\n",
       "      <td>11.991312</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.823985</td>\n",
       "      <td>6.344241</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>8.644899</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.742248</td>\n",
       "      <td>9.290322</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>13.341740</td>\n",
       "      <td>0.724709</td>\n",
       "      <td>9.922503</td>\n",
       "      <td>24.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>12.061029</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.831721</td>\n",
       "      <td>6.065373</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.752418</td>\n",
       "      <td>8.923767</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.715116</td>\n",
       "      <td>10.268250</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.631783</td>\n",
       "      <td>13.271888</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>10.118061</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.663443</td>\n",
       "      <td>12.130746</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.839458</td>\n",
       "      <td>5.786505</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>9.202635</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>11.455735</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.713082</td>\n",
       "      <td>10.341561</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.661509</td>\n",
       "      <td>12.200463</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.841393</td>\n",
       "      <td>5.716788</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>9.830087</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>12.573367</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.703399</td>\n",
       "      <td>10.690578</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.653772</td>\n",
       "      <td>12.479331</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.827853</td>\n",
       "      <td>6.204807</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.709865</td>\n",
       "      <td>10.457540</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.625969</td>\n",
       "      <td>13.481444</td>\n",
       "      <td>0.690623</td>\n",
       "      <td>11.151061</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.655706</td>\n",
       "      <td>12.409614</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>6.553392</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.705996</td>\n",
       "      <td>10.596974</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.637597</td>\n",
       "      <td>13.062332</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.608527</td>\n",
       "      <td>14.110112</td>\n",
       "      <td>0.685202</td>\n",
       "      <td>11.346485</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.647969</td>\n",
       "      <td>12.688481</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>6.901976</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.704062</td>\n",
       "      <td>10.666690</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.620155</td>\n",
       "      <td>13.691000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>14.808633</td>\n",
       "      <td>0.673969</td>\n",
       "      <td>11.751356</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.657640</td>\n",
       "      <td>12.339897</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.823985</td>\n",
       "      <td>6.344241</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.713733</td>\n",
       "      <td>10.318106</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.624031</td>\n",
       "      <td>13.551296</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.548450</td>\n",
       "      <td>16.275526</td>\n",
       "      <td>0.673568</td>\n",
       "      <td>11.765813</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>12.061029</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.833656</td>\n",
       "      <td>5.995656</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.704062</td>\n",
       "      <td>10.666690</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.618217</td>\n",
       "      <td>13.760852</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.560078</td>\n",
       "      <td>15.856413</td>\n",
       "      <td>0.676278</td>\n",
       "      <td>11.668128</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.671180</td>\n",
       "      <td>11.851878</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.692456</td>\n",
       "      <td>11.084992</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>14.179964</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.556202</td>\n",
       "      <td>15.996117</td>\n",
       "      <td>0.668148</td>\n",
       "      <td>11.961156</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.696325</td>\n",
       "      <td>10.945558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756286</td>\n",
       "      <td>8.784333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.704062</td>\n",
       "      <td>10.666690</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600775</td>\n",
       "      <td>14.389521</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.536822</td>\n",
       "      <td>16.694638</td>\n",
       "      <td>0.658854</td>\n",
       "      <td>12.296148</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.690522</td>\n",
       "      <td>11.154709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605416</td>\n",
       "      <td>14.222254</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>15.367449</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.523256</td>\n",
       "      <td>17.183602</td>\n",
       "      <td>0.569090</td>\n",
       "      <td>15.531581</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split_0                         split_1                         split_2   \n",
       "       dof  accuracy cross-entropy     dof  accuracy cross-entropy     dof   \n",
       "0     33.0  0.837524      5.856222    33.0  0.765957      8.435749    33.0  \\\n",
       "1     33.0  0.837524      5.856222    33.0  0.765957      8.435749    33.0   \n",
       "2     33.0  0.837524      5.856222    33.0  0.767892      8.366032    33.0   \n",
       "3     33.0  0.839458      5.786505    33.0  0.767892      8.366032    33.0   \n",
       "4     33.0  0.839458      5.786505    32.0  0.767892      8.366032    32.0   \n",
       "5     33.0  0.837524      5.856222    32.0  0.767892      8.366032    32.0   \n",
       "6     33.0  0.837524      5.856222    32.0  0.767892      8.366032    32.0   \n",
       "7     33.0  0.829787      6.135090    32.0  0.769826      8.296315    32.0   \n",
       "8     33.0  0.825919      6.274524    32.0  0.771760      8.226598    32.0   \n",
       "9     32.0  0.822050      6.413958    31.0  0.777563      8.017447    31.0   \n",
       "10    31.0  0.764023      8.505466    31.0  0.785300      7.738579    31.0   \n",
       "11    31.0  0.729207      9.760370    31.0  0.783366      7.808296    31.0   \n",
       "12    31.0  0.688588     11.224426    30.0  0.791103      7.529429    31.0   \n",
       "13    30.0  0.644101     12.827915    29.0  0.793037      7.459712    31.0   \n",
       "14    30.0  0.644101     12.827915    29.0  0.789168      7.599145    29.0   \n",
       "15    29.0  0.651838     12.549048    29.0  0.804642      7.041410    29.0   \n",
       "16    26.0  0.655706     12.409614    29.0  0.806576      6.971693    29.0   \n",
       "17    25.0  0.657640     12.339897    29.0  0.816248      6.623108    28.0   \n",
       "18    25.0  0.665377     12.061029    28.0  0.823985      6.344241    26.0   \n",
       "19    22.0  0.667311     11.991312    26.0  0.823985      6.344241    25.0   \n",
       "20    22.0  0.665377     12.061029    22.0  0.831721      6.065373    23.0   \n",
       "21    20.0  0.663443     12.130746    20.0  0.839458      5.786505    19.0   \n",
       "22    16.0  0.661509     12.200463    19.0  0.841393      5.716788    17.0   \n",
       "23    15.0  0.653772     12.479331    16.0  0.827853      6.204807    16.0   \n",
       "24    13.0  0.655706     12.409614    14.0  0.818182      6.553392    12.0   \n",
       "25    11.0  0.647969     12.688481    11.0  0.808511      6.901976    10.0   \n",
       "26    10.0  0.657640     12.339897     8.0  0.823985      6.344241     9.0   \n",
       "27     5.0  0.665377     12.061029     5.0  0.833656      5.995656     6.0   \n",
       "28     4.0  0.671180     11.851878     3.0  0.814313      6.692825     4.0   \n",
       "29     2.0  0.696325     10.945558     1.0  0.756286      8.784333     3.0   \n",
       "30     1.0  0.690522     11.154709     0.0  0.452611     19.729892     1.0   \n",
       "31     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "32     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "33     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "34     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "35     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "36     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "37     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "38     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "39     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "40     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "41     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "42     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "43     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "44     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "45     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "46     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "47     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "48     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "49     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "\n",
       "                           split_3                         split_4             \n",
       "    accuracy cross-entropy     dof  accuracy cross-entropy     dof  accuracy   \n",
       "0   0.814313      6.692825    33.0  0.874031      4.540383    32.0  0.656977  \\\n",
       "1   0.814313      6.692825    33.0  0.874031      4.540383    32.0  0.656977   \n",
       "2   0.814313      6.692825    33.0  0.874031      4.540383    32.0  0.656977   \n",
       "3   0.814313      6.692825    33.0  0.872093      4.610235    32.0  0.656977   \n",
       "4   0.814313      6.692825    33.0  0.872093      4.610235    32.0  0.656977   \n",
       "5   0.814313      6.692825    33.0  0.872093      4.610235    32.0  0.656977   \n",
       "6   0.816248      6.623108    33.0  0.872093      4.610235    32.0  0.656977   \n",
       "7   0.816248      6.623108    33.0  0.872093      4.610235    32.0  0.656977   \n",
       "8   0.816248      6.623108    32.0  0.872093      4.610235    32.0  0.656977   \n",
       "9   0.816248      6.623108    32.0  0.870155      4.680087    31.0  0.656977   \n",
       "10  0.810445      6.832259    31.0  0.868217      4.749939    30.0  0.653101   \n",
       "11  0.806576      6.971693    30.0  0.858527      5.099199    28.0  0.649225   \n",
       "12  0.812379      6.762542    30.0  0.852713      5.308755    28.0  0.649225   \n",
       "13  0.806576      6.971693    30.0  0.844961      5.588163    28.0  0.647287   \n",
       "14  0.802708      7.111127    29.0  0.839147      5.797719    28.0  0.639535   \n",
       "15  0.796905      7.320278    28.0  0.831395      6.077128    28.0  0.635659   \n",
       "16  0.793037      7.459712    28.0  0.815891      6.635944    27.0  0.635659   \n",
       "17  0.781431      7.878013    26.0  0.804264      7.055056    25.0  0.635659   \n",
       "18  0.760155      8.644899    25.0  0.777132      8.032985    24.0  0.635659   \n",
       "19  0.760155      8.644899    25.0  0.742248      9.290322    24.0  0.629845   \n",
       "20  0.752418      8.923767    22.0  0.715116     10.268250    21.0  0.631783   \n",
       "21  0.744681      9.202635    20.0  0.682171     11.455735    19.0  0.635659   \n",
       "22  0.727273      9.830087    17.0  0.651163     12.573367    17.0  0.635659   \n",
       "23  0.709865     10.457540    13.0  0.635659     13.132184    14.0  0.625969   \n",
       "24  0.705996     10.596974    12.0  0.637597     13.062332    13.0  0.608527   \n",
       "25  0.704062     10.666690    10.0  0.620155     13.691000    11.0  0.589147   \n",
       "26  0.713733     10.318106     7.0  0.624031     13.551296     7.0  0.548450   \n",
       "27  0.704062     10.666690     6.0  0.618217     13.760852     5.0  0.560078   \n",
       "28  0.692456     11.084992     4.0  0.606589     14.179964     4.0  0.556202   \n",
       "29  0.704062     10.666690     3.0  0.600775     14.389521     3.0  0.536822   \n",
       "30  0.605416     14.222254     3.0  0.573643     15.367449     2.0  0.523256   \n",
       "31  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "32  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "33  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "34  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "35  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "36  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "37  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "38  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "39  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "40  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "41  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "42  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "43  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "44  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "45  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "46  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "47  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "48  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "49  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "\n",
       "                      mean                      \n",
       "   cross-entropy  accuracy cross-entropy   dof  \n",
       "0      12.363811  0.789761      7.577798  32.8  \n",
       "1      12.363811  0.789761      7.577798  32.8  \n",
       "2      12.363811  0.790147      7.563855  32.8  \n",
       "3      12.363811  0.790147      7.563882  32.8  \n",
       "4      12.363811  0.790147      7.563882  32.4  \n",
       "5      12.363811  0.789760      7.577825  32.4  \n",
       "6      12.363811  0.790147      7.563882  32.4  \n",
       "7      12.363811  0.788986      7.605712  32.4  \n",
       "8      12.363811  0.788599      7.619655  32.2  \n",
       "9      12.363811  0.788599      7.619682  31.4  \n",
       "10     12.503515  0.776217      8.065952  30.8  \n",
       "11     12.643220  0.765380      8.456556  30.2  \n",
       "12     12.643220  0.758802      8.693674  30.0  \n",
       "13     12.713072  0.747192      9.112111  29.6  \n",
       "14     12.992480  0.742932      9.265677  29.0  \n",
       "15     13.132184  0.744088      9.224009  28.6  \n",
       "16     13.132184  0.741374      9.321829  27.8  \n",
       "17     13.132184  0.739048      9.405652  26.6  \n",
       "18     13.132184  0.732461      9.643068  25.6  \n",
       "19     13.341740  0.724709      9.922503  24.4  \n",
       "20     13.271888  0.719283     10.118061  22.0  \n",
       "21     13.132184  0.713082     10.341561  19.6  \n",
       "22     13.132184  0.703399     10.690578  17.2  \n",
       "23     13.481444  0.690623     11.151061  14.8  \n",
       "24     14.110112  0.685202     11.346485  12.8  \n",
       "25     14.808633  0.673969     11.751356  10.6  \n",
       "26     16.275526  0.673568     11.765813   8.2  \n",
       "27     15.856413  0.676278     11.668128   5.4  \n",
       "28     15.996117  0.668148     11.961156   3.8  \n",
       "29     16.694638  0.658854     12.296148   2.4  \n",
       "30     17.183602  0.569090     15.531581   1.4  \n",
       "31      4.400679  0.591360     14.728864   0.0  \n",
       "32      4.400679  0.591360     14.728864   0.0  \n",
       "33      4.400679  0.591360     14.728864   0.0  \n",
       "34      4.400679  0.591360     14.728864   0.0  \n",
       "35      4.400679  0.591360     14.728864   0.0  \n",
       "36      4.400679  0.591360     14.728864   0.0  \n",
       "37      4.400679  0.591360     14.728864   0.0  \n",
       "38      4.400679  0.591360     14.728864   0.0  \n",
       "39      4.400679  0.591360     14.728864   0.0  \n",
       "40      4.400679  0.591360     14.728864   0.0  \n",
       "41      4.400679  0.591360     14.728864   0.0  \n",
       "42      4.400679  0.591360     14.728864   0.0  \n",
       "43      4.400679  0.591360     14.728864   0.0  \n",
       "44      4.400679  0.591360     14.728864   0.0  \n",
       "45      4.400679  0.591360     14.728864   0.0  \n",
       "46      4.400679  0.591360     14.728864   0.0  \n",
       "47      4.400679  0.591360     14.728864   0.0  \n",
       "48      4.400679  0.591360     14.728864   0.0  \n",
       "49      4.400679  0.591360     14.728864   0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_linreg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a51b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.7901473917802468 std: 0.054277553922625926 thresh: 0.735869837857621\n",
      "Best average fit: split_0  dof              25.000000\n",
      "         accuracy          0.657640\n",
      "         cross-entropy    12.339897\n",
      "split_1  dof              29.000000\n",
      "         accuracy          0.816248\n",
      "         cross-entropy     6.623108\n",
      "split_2  dof              28.000000\n",
      "         accuracy          0.781431\n",
      "         cross-entropy     7.878013\n",
      "split_3  dof              26.000000\n",
      "         accuracy          0.804264\n",
      "         cross-entropy     7.055056\n",
      "split_4  dof              25.000000\n",
      "         accuracy          0.635659\n",
      "         cross-entropy    13.132184\n",
      "mean     accuracy          0.739048\n",
      "         cross-entropy     9.405652\n",
      "         dof              26.600000\n",
      "Name: 17, dtype: float64\n",
      "Best parameter: 0.08760636408778617\n"
     ]
    }
   ],
   "source": [
    "sk_best_reg = find_optimal_param(sk_linreg_res, sk_reg_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5730c",
   "metadata": {},
   "source": [
    "## glmnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glmnet import LogitNet\n",
    "\n",
    "glmnet_reg_params = np.concatenate((np.linspace(1,10,10)[::-1], np.logspace(1, 15, 20, base=0.5)))\n",
    "cv_results = dict()\n",
    "\n",
    "for i, (cv_train_idx, cv_test_idx) in enumerate(kf.split(X_train_bin, y_train_elong)):\n",
    "    \n",
    "    s = \"split_%s\" % i\n",
    "    print(s)\n",
    "\n",
    "    cv_results[s] = dict()\n",
    "    cv_results[s][\"dof\"] = []\n",
    "    for scorer in scorers.keys():\n",
    "        cv_results[s][scorer] = []\n",
    "    \n",
    "    for l in glmnet_reg_params:\n",
    "        model = LogitNet(alpha=1,\n",
    "                         lambda_path=[l],\n",
    "                         fit_intercept=False,\n",
    "                         standardize=False,\n",
    "                         random_state=1234,\n",
    "                         max_iter=10000)\n",
    "\n",
    "        model.fit(X_train_bin[cv_train_idx,] , y_train_bin[cv_train_idx])\n",
    "\n",
    "        for scorer in scorers.keys():\n",
    "            predicted = model.predict(X_train_bin[cv_test_idx,])            \n",
    "            score = scorers[scorer](y_train_bin[cv_test_idx], predicted)\n",
    "            cv_results[s][scorer].append(score)\n",
    "\n",
    "        cv_results[s][\"dof\"].append(dof(model.coef_))\n",
    "\n",
    "glmnet_cv_res = cv_res_to_df(cv_results, scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa033b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "glmnet_cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95550326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "glmnet_best_reg = find_optimal_param(glmnet_cv_res, glmnet_reg_params, lower_increases_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce492e9d",
   "metadata": {},
   "source": [
    "## SGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ebc3f20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_0\n",
      "split_1\n",
      "split_2\n",
      "split_3\n",
      "split_4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_reg_params = reg_params\n",
    "cv_results = dict()\n",
    "\n",
    "# fixed model training params\n",
    "max_iter = 50\n",
    "\n",
    "for i, (cv_train_idx, cv_test_idx) in enumerate(kf.split(X_train_bin, y_train_elong)):\n",
    "    \n",
    "    s = \"split_%s\" % i\n",
    "    print(s)\n",
    "\n",
    "    cv_results[s] = dict()\n",
    "    cv_results[s][\"dof\"] = []\n",
    "    for scorer in scorers.keys():\n",
    "        cv_results[s][scorer] = []\n",
    "    \n",
    "    for a in sgd_reg_params:\n",
    "        \n",
    "        model = SGDClassifier(loss=\"log_loss\",\n",
    "                              penalty=\"l1\",\n",
    "                              alpha=a,  # = lambda in paper!! Very important to tune for the desired sparsity!\n",
    "                              fit_intercept=False,\n",
    "                              random_state=121,\n",
    "                              n_jobs=1)\n",
    "        cur_iter = 0\n",
    "\n",
    "        while cur_iter < max_iter:\n",
    "            cur_iter += 1\n",
    "\n",
    "            # fit from samples of the big matrix\n",
    "            # TODO: Sampling from the big matrix directly is just for PoP,\n",
    "            # and eliminates the purpose. Only the binarized y-vector should\n",
    "            # be created and the indexes taken from the log count matrix.\n",
    "            rng.shuffle(cv_train_idx)\n",
    "            model.partial_fit(X_train_bin[cv_train_idx,], y_train_bin[cv_train_idx], classes=np.unique(y_batch))\n",
    "\n",
    "        for scorer in scorers.keys():\n",
    "            predicted = model.predict(X_train_bin[cv_test_idx,])            \n",
    "            score = scorers[scorer](y_train_bin[cv_test_idx], predicted)\n",
    "            cv_results[s][scorer].append(score)\n",
    "\n",
    "        cv_results[s][\"dof\"].append(dof(model.coef_))\n",
    "\n",
    "sgd_cv_res = cv_res_to_df(cv_results, scorers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "954db7a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_3</th>\n",
       "      <th colspan=\"3\" halign=\"left\">split_4</th>\n",
       "      <th colspan=\"3\" halign=\"left\">mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cross-entropy</th>\n",
       "      <th>dof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210832</td>\n",
       "      <td>28.444508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>7.613873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.877907</td>\n",
       "      <td>4.400679</td>\n",
       "      <td>0.591360</td>\n",
       "      <td>14.728864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.690522</td>\n",
       "      <td>11.154709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.452611</td>\n",
       "      <td>19.729892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626692</td>\n",
       "      <td>13.455368</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.515504</td>\n",
       "      <td>17.463010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.527132</td>\n",
       "      <td>17.043898</td>\n",
       "      <td>0.562492</td>\n",
       "      <td>15.769375</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.698259</td>\n",
       "      <td>10.875841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756286</td>\n",
       "      <td>8.784333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.705996</td>\n",
       "      <td>10.596974</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.602713</td>\n",
       "      <td>14.319668</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.532946</td>\n",
       "      <td>16.834342</td>\n",
       "      <td>0.659240</td>\n",
       "      <td>12.282232</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.675048</td>\n",
       "      <td>11.712444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>6.901976</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.696325</td>\n",
       "      <td>10.945558</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>14.179964</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.554264</td>\n",
       "      <td>16.065970</td>\n",
       "      <td>0.668147</td>\n",
       "      <td>11.961183</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>12.061029</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.831721</td>\n",
       "      <td>6.065373</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.704062</td>\n",
       "      <td>10.666690</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.618217</td>\n",
       "      <td>13.760852</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.560078</td>\n",
       "      <td>15.856413</td>\n",
       "      <td>0.675891</td>\n",
       "      <td>11.682072</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.657640</td>\n",
       "      <td>12.339897</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.823985</td>\n",
       "      <td>6.344241</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.713733</td>\n",
       "      <td>10.318106</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.624031</td>\n",
       "      <td>13.551296</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>16.345378</td>\n",
       "      <td>0.673180</td>\n",
       "      <td>11.779783</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.647969</td>\n",
       "      <td>12.688481</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>6.901976</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.700193</td>\n",
       "      <td>10.806124</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.620155</td>\n",
       "      <td>13.691000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.579457</td>\n",
       "      <td>15.157893</td>\n",
       "      <td>0.671257</td>\n",
       "      <td>11.849095</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.657640</td>\n",
       "      <td>12.339897</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.709865</td>\n",
       "      <td>10.457540</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.633721</td>\n",
       "      <td>13.202036</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>14.249816</td>\n",
       "      <td>0.684038</td>\n",
       "      <td>11.388423</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.655706</td>\n",
       "      <td>12.409614</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>6.135090</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.713733</td>\n",
       "      <td>10.318106</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>12.992480</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.625969</td>\n",
       "      <td>13.481444</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>11.067347</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>12.061029</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.841393</td>\n",
       "      <td>5.716788</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>9.830087</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>12.713072</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.637597</td>\n",
       "      <td>13.062332</td>\n",
       "      <td>0.703785</td>\n",
       "      <td>10.676662</td>\n",
       "      <td>16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.663443</td>\n",
       "      <td>12.130746</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.839458</td>\n",
       "      <td>5.786505</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.742747</td>\n",
       "      <td>9.272352</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>11.455735</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.712695</td>\n",
       "      <td>10.355504</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.671180</td>\n",
       "      <td>11.851878</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.835590</td>\n",
       "      <td>5.925939</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.750484</td>\n",
       "      <td>8.993484</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.715116</td>\n",
       "      <td>10.268250</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>13.341740</td>\n",
       "      <td>0.720443</td>\n",
       "      <td>10.076258</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.667311</td>\n",
       "      <td>11.991312</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>6.135090</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.758221</td>\n",
       "      <td>8.714616</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>9.220469</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>13.341740</td>\n",
       "      <td>0.725870</td>\n",
       "      <td>9.880646</td>\n",
       "      <td>24.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.665377</td>\n",
       "      <td>12.061029</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>6.413958</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>8.644899</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.792636</td>\n",
       "      <td>7.474168</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>13.411592</td>\n",
       "      <td>0.733625</td>\n",
       "      <td>9.601129</td>\n",
       "      <td>26.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>12.270180</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>6.413958</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.779497</td>\n",
       "      <td>7.947730</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.806202</td>\n",
       "      <td>6.985204</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>13.132184</td>\n",
       "      <td>0.740596</td>\n",
       "      <td>9.349851</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.640232</td>\n",
       "      <td>12.967349</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.820116</td>\n",
       "      <td>6.483675</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.793037</td>\n",
       "      <td>7.459712</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.817829</td>\n",
       "      <td>6.566092</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.637597</td>\n",
       "      <td>13.062332</td>\n",
       "      <td>0.741762</td>\n",
       "      <td>9.307832</td>\n",
       "      <td>28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.644101</td>\n",
       "      <td>12.827915</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.798839</td>\n",
       "      <td>7.250561</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>7.599145</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>5.658015</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.641473</td>\n",
       "      <td>12.922628</td>\n",
       "      <td>0.743321</td>\n",
       "      <td>9.251653</td>\n",
       "      <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.661509</td>\n",
       "      <td>12.200463</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>7.599145</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.796905</td>\n",
       "      <td>7.320278</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.866279</td>\n",
       "      <td>4.819791</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.641473</td>\n",
       "      <td>12.922628</td>\n",
       "      <td>0.751067</td>\n",
       "      <td>8.972461</td>\n",
       "      <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>13.037066</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.783366</td>\n",
       "      <td>7.808296</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>4.889643</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>12.713072</td>\n",
       "      <td>0.749521</td>\n",
       "      <td>9.028180</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.667311</td>\n",
       "      <td>11.991312</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.779497</td>\n",
       "      <td>7.947730</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.804642</td>\n",
       "      <td>7.041410</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.844961</td>\n",
       "      <td>5.588163</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.637597</td>\n",
       "      <td>13.062332</td>\n",
       "      <td>0.746802</td>\n",
       "      <td>9.126189</td>\n",
       "      <td>30.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.705996</td>\n",
       "      <td>10.596974</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.758221</td>\n",
       "      <td>8.714616</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.839147</td>\n",
       "      <td>5.797719</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>12.154255</td>\n",
       "      <td>0.756094</td>\n",
       "      <td>8.791278</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.754352</td>\n",
       "      <td>8.854050</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>7.599145</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.812379</td>\n",
       "      <td>6.762542</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>5.029347</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>12.014551</td>\n",
       "      <td>0.776606</td>\n",
       "      <td>8.051927</td>\n",
       "      <td>31.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.796905</td>\n",
       "      <td>7.320278</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.758221</td>\n",
       "      <td>8.714616</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>6.413958</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.866279</td>\n",
       "      <td>4.819791</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.670543</td>\n",
       "      <td>11.874847</td>\n",
       "      <td>0.782800</td>\n",
       "      <td>7.828698</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.820116</td>\n",
       "      <td>6.483675</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.764023</td>\n",
       "      <td>8.505466</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.798839</td>\n",
       "      <td>7.250561</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>4.889643</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.643411</td>\n",
       "      <td>12.852776</td>\n",
       "      <td>0.778146</td>\n",
       "      <td>7.996424</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.839458</td>\n",
       "      <td>5.786505</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.769826</td>\n",
       "      <td>8.296315</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.804642</td>\n",
       "      <td>7.041410</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.889535</td>\n",
       "      <td>3.981566</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.649225</td>\n",
       "      <td>12.643220</td>\n",
       "      <td>0.790537</td>\n",
       "      <td>7.549803</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>5.368204</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.810445</td>\n",
       "      <td>6.832259</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.812379</td>\n",
       "      <td>6.762542</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.870155</td>\n",
       "      <td>4.680087</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.637597</td>\n",
       "      <td>13.062332</td>\n",
       "      <td>0.796328</td>\n",
       "      <td>7.341085</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.831721</td>\n",
       "      <td>6.065373</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.646035</td>\n",
       "      <td>12.758198</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.791103</td>\n",
       "      <td>7.529429</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>5.867571</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.656977</td>\n",
       "      <td>12.363811</td>\n",
       "      <td>0.752609</td>\n",
       "      <td>8.916877</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.779497</td>\n",
       "      <td>7.947730</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>8.644899</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>9.202635</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.817829</td>\n",
       "      <td>6.566092</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>12.573367</td>\n",
       "      <td>0.750665</td>\n",
       "      <td>8.986945</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>6.135090</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.686654</td>\n",
       "      <td>11.294143</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.771760</td>\n",
       "      <td>8.226598</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>5.448459</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>12.992480</td>\n",
       "      <td>0.755315</td>\n",
       "      <td>8.819354</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>6.413958</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.692456</td>\n",
       "      <td>11.084992</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.796905</td>\n",
       "      <td>7.320278</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>5.308755</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>13.341740</td>\n",
       "      <td>0.758794</td>\n",
       "      <td>8.693945</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.798839</td>\n",
       "      <td>7.250561</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.686654</td>\n",
       "      <td>11.294143</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>6.901976</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.874031</td>\n",
       "      <td>4.540383</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.649225</td>\n",
       "      <td>12.643220</td>\n",
       "      <td>0.763452</td>\n",
       "      <td>8.526056</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.814313</td>\n",
       "      <td>6.692825</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.802708</td>\n",
       "      <td>7.111127</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.771318</td>\n",
       "      <td>8.242541</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>13.411592</td>\n",
       "      <td>0.766499</td>\n",
       "      <td>8.416239</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.823985</td>\n",
       "      <td>6.344241</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.806576</td>\n",
       "      <td>6.971693</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.669246</td>\n",
       "      <td>11.921595</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.854651</td>\n",
       "      <td>5.238903</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.624031</td>\n",
       "      <td>13.551296</td>\n",
       "      <td>0.755698</td>\n",
       "      <td>8.805546</td>\n",
       "      <td>32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>6.553392</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.715667</td>\n",
       "      <td>10.248389</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.827853</td>\n",
       "      <td>6.204807</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.866279</td>\n",
       "      <td>4.819791</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>12.154255</td>\n",
       "      <td>0.778154</td>\n",
       "      <td>7.996127</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.816248</td>\n",
       "      <td>6.623108</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.754352</td>\n",
       "      <td>8.854050</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>7.599145</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>8.102837</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>14.668929</td>\n",
       "      <td>0.745597</td>\n",
       "      <td>9.169614</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>6.413958</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.733075</td>\n",
       "      <td>9.620936</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>7.599145</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>5.727867</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.651163</td>\n",
       "      <td>12.573367</td>\n",
       "      <td>0.767308</td>\n",
       "      <td>8.387055</td>\n",
       "      <td>32.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split_0                         split_1                         split_2   \n",
       "       dof  accuracy cross-entropy     dof  accuracy cross-entropy     dof   \n",
       "0      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0  \\\n",
       "1      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "2      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "3      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "4      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "5      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "6      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "7      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "8      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "9      0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "10     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "11     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "12     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "13     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "14     0.0  0.210832     28.444508     0.0  0.452611     19.729892     0.0   \n",
       "15     1.0  0.690522     11.154709     0.0  0.452611     19.729892     0.0   \n",
       "16     2.0  0.698259     10.875841     1.0  0.756286      8.784333     3.0   \n",
       "17     4.0  0.675048     11.712444     2.0  0.808511      6.901976     4.0   \n",
       "18     5.0  0.665377     12.061029     5.0  0.831721      6.065373     6.0   \n",
       "19    10.0  0.657640     12.339897     8.0  0.823985      6.344241     8.0   \n",
       "20    11.0  0.647969     12.688481    10.0  0.808511      6.901976    10.0   \n",
       "21    13.0  0.657640     12.339897    14.0  0.814313      6.692825    12.0   \n",
       "22    15.0  0.655706     12.409614    16.0  0.829787      6.135090    16.0   \n",
       "23    16.0  0.665377     12.061029    18.0  0.841393      5.716788    17.0   \n",
       "24    20.0  0.663443     12.130746    20.0  0.839458      5.786505    20.0   \n",
       "25    22.0  0.671180     11.851878    23.0  0.835590      5.925939    23.0   \n",
       "26    22.0  0.667311     11.991312    26.0  0.829787      6.135090    25.0   \n",
       "27    25.0  0.665377     12.061029    29.0  0.822050      6.413958    26.0   \n",
       "28    25.0  0.659574     12.270180    29.0  0.822050      6.413958    28.0   \n",
       "29    28.0  0.640232     12.967349    30.0  0.820116      6.483675    29.0   \n",
       "30    31.0  0.644101     12.827915    29.0  0.798839      7.250561    30.0   \n",
       "31    29.0  0.661509     12.200463    29.0  0.789168      7.599145    31.0   \n",
       "32    31.0  0.638298     13.037066    30.0  0.783366      7.808296    32.0   \n",
       "33    31.0  0.667311     11.991312    30.0  0.779497      7.947730    31.0   \n",
       "34    32.0  0.705996     10.596974    32.0  0.758221      8.714616    32.0   \n",
       "35    32.0  0.754352      8.854050    31.0  0.789168      7.599145    33.0   \n",
       "36    31.0  0.796905      7.320278    33.0  0.758221      8.714616    32.0   \n",
       "37    33.0  0.820116      6.483675    33.0  0.764023      8.505466    31.0   \n",
       "38    31.0  0.839458      5.786505    33.0  0.769826      8.296315    32.0   \n",
       "39    31.0  0.851064      5.368204    32.0  0.810445      6.832259    32.0   \n",
       "40    33.0  0.831721      6.065373    32.0  0.646035     12.758198    32.0   \n",
       "41    32.0  0.779497      7.947730    32.0  0.760155      8.644899    33.0   \n",
       "42    33.0  0.829787      6.135090    33.0  0.686654     11.294143    33.0   \n",
       "43    33.0  0.822050      6.413958    33.0  0.692456     11.084992    32.0   \n",
       "44    33.0  0.798839      7.250561    33.0  0.686654     11.294143    33.0   \n",
       "45    33.0  0.816248      6.623108    32.0  0.814313      6.692825    33.0   \n",
       "46    33.0  0.823985      6.344241    32.0  0.806576      6.971693    33.0   \n",
       "47    33.0  0.818182      6.553392    33.0  0.715667     10.248389    33.0   \n",
       "48    33.0  0.816248      6.623108    33.0  0.754352      8.854050    33.0   \n",
       "49    33.0  0.822050      6.413958    33.0  0.733075      9.620936    33.0   \n",
       "\n",
       "                           split_3                         split_4             \n",
       "    accuracy cross-entropy     dof  accuracy cross-entropy     dof  accuracy   \n",
       "0   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907  \\\n",
       "1   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "2   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "3   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "4   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "5   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "6   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "7   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "8   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "9   0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "10  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "11  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "12  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "13  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "14  0.626692     13.455368     0.0  0.788760      7.613873     0.0  0.877907   \n",
       "15  0.626692     13.455368     3.0  0.515504     17.463010     2.0  0.527132   \n",
       "16  0.705996     10.596974     3.0  0.602713     14.319668     3.0  0.532946   \n",
       "17  0.696325     10.945558     4.0  0.606589     14.179964     4.0  0.554264   \n",
       "18  0.704062     10.666690     6.0  0.618217     13.760852     4.0  0.560078   \n",
       "19  0.713733     10.318106     7.0  0.624031     13.551296     7.0  0.546512   \n",
       "20  0.700193     10.806124    10.0  0.620155     13.691000    10.0  0.579457   \n",
       "21  0.709865     10.457540    12.0  0.633721     13.202036    13.0  0.604651   \n",
       "22  0.713733     10.318106    13.0  0.639535     12.992480    14.0  0.625969   \n",
       "23  0.727273      9.830087    15.0  0.647287     12.713072    15.0  0.637597   \n",
       "24  0.742747      9.272352    20.0  0.682171     11.455735    20.0  0.635659   \n",
       "25  0.750484      8.993484    22.0  0.715116     10.268250    22.0  0.629845   \n",
       "26  0.758221      8.714616    25.0  0.744186      9.220469    24.0  0.629845   \n",
       "27  0.760155      8.644899    25.0  0.792636      7.474168    27.0  0.627907   \n",
       "28  0.779497      7.947730    27.0  0.806202      6.985204    26.0  0.635659   \n",
       "29  0.793037      7.459712    28.0  0.817829      6.566092    28.0  0.637597   \n",
       "30  0.789168      7.599145    29.0  0.843023      5.658015    28.0  0.641473   \n",
       "31  0.796905      7.320278    30.0  0.866279      4.819791    28.0  0.641473   \n",
       "32  0.814313      6.692825    30.0  0.864341      4.889643    30.0  0.647287   \n",
       "33  0.804642      7.041410    29.0  0.844961      5.588163    31.0  0.637597   \n",
       "34  0.814313      6.692825    32.0  0.839147      5.797719    29.0  0.662791   \n",
       "35  0.812379      6.762542    33.0  0.860465      5.029347    29.0  0.666667   \n",
       "36  0.822050      6.413958    32.0  0.866279      4.819791    32.0  0.670543   \n",
       "37  0.798839      7.250561    31.0  0.864341      4.889643    29.0  0.643411   \n",
       "38  0.804642      7.041410    32.0  0.889535      3.981566    29.0  0.649225   \n",
       "39  0.812379      6.762542    32.0  0.870155      4.680087    30.0  0.637597   \n",
       "40  0.791103      7.529429    32.0  0.837209      5.867571    32.0  0.656977   \n",
       "41  0.744681      9.202635    32.0  0.817829      6.566092    32.0  0.651163   \n",
       "42  0.771760      8.226598    32.0  0.848837      5.448459    32.0  0.639535   \n",
       "43  0.796905      7.320278    33.0  0.852713      5.308755    31.0  0.629845   \n",
       "44  0.808511      6.901976    33.0  0.874031      4.540383    32.0  0.649225   \n",
       "45  0.802708      7.111127    32.0  0.771318      8.242541    32.0  0.627907   \n",
       "46  0.669246     11.921595    33.0  0.854651      5.238903    32.0  0.624031   \n",
       "47  0.827853      6.204807    33.0  0.866279      4.819791    32.0  0.662791   \n",
       "48  0.789168      7.599145    33.0  0.775194      8.102837    32.0  0.593023   \n",
       "49  0.789168      7.599145    33.0  0.841085      5.727867    32.0  0.651163   \n",
       "\n",
       "                      mean                      \n",
       "   cross-entropy  accuracy cross-entropy   dof  \n",
       "0       4.400679  0.591360     14.728864   0.0  \n",
       "1       4.400679  0.591360     14.728864   0.0  \n",
       "2       4.400679  0.591360     14.728864   0.0  \n",
       "3       4.400679  0.591360     14.728864   0.0  \n",
       "4       4.400679  0.591360     14.728864   0.0  \n",
       "5       4.400679  0.591360     14.728864   0.0  \n",
       "6       4.400679  0.591360     14.728864   0.0  \n",
       "7       4.400679  0.591360     14.728864   0.0  \n",
       "8       4.400679  0.591360     14.728864   0.0  \n",
       "9       4.400679  0.591360     14.728864   0.0  \n",
       "10      4.400679  0.591360     14.728864   0.0  \n",
       "11      4.400679  0.591360     14.728864   0.0  \n",
       "12      4.400679  0.591360     14.728864   0.0  \n",
       "13      4.400679  0.591360     14.728864   0.0  \n",
       "14      4.400679  0.591360     14.728864   0.0  \n",
       "15     17.043898  0.562492     15.769375   1.2  \n",
       "16     16.834342  0.659240     12.282232   2.4  \n",
       "17     16.065970  0.668147     11.961183   3.6  \n",
       "18     15.856413  0.675891     11.682072   5.2  \n",
       "19     16.345378  0.673180     11.779783   8.0  \n",
       "20     15.157893  0.671257     11.849095  10.2  \n",
       "21     14.249816  0.684038     11.388423  12.8  \n",
       "22     13.481444  0.692946     11.067347  14.8  \n",
       "23     13.062332  0.703785     10.676662  16.2  \n",
       "24     13.132184  0.712695     10.355504  20.0  \n",
       "25     13.341740  0.720443     10.076258  22.4  \n",
       "26     13.341740  0.725870      9.880646  24.4  \n",
       "27     13.411592  0.733625      9.601129  26.4  \n",
       "28     13.132184  0.740596      9.349851  27.0  \n",
       "29     13.062332  0.741762      9.307832  28.6  \n",
       "30     12.922628  0.743321      9.251653  29.4  \n",
       "31     12.922628  0.751067      8.972461  29.4  \n",
       "32     12.713072  0.749521      9.028180  30.6  \n",
       "33     13.062332  0.746802      9.126189  30.4  \n",
       "34     12.154255  0.756094      8.791278  31.4  \n",
       "35     12.014551  0.776606      8.051927  31.6  \n",
       "36     11.874847  0.782800      7.828698  32.0  \n",
       "37     12.852776  0.778146      7.996424  31.4  \n",
       "38     12.643220  0.790537      7.549803  31.4  \n",
       "39     13.062332  0.796328      7.341085  31.4  \n",
       "40     12.363811  0.752609      8.916877  32.2  \n",
       "41     12.573367  0.750665      8.986945  32.2  \n",
       "42     12.992480  0.755315      8.819354  32.6  \n",
       "43     13.341740  0.758794      8.693945  32.4  \n",
       "44     12.643220  0.763452      8.526056  32.8  \n",
       "45     13.411592  0.766499      8.416239  32.4  \n",
       "46     13.551296  0.755698      8.805546  32.6  \n",
       "47     12.154255  0.778154      7.996127  32.8  \n",
       "48     14.668929  0.745597      9.169614  32.8  \n",
       "49     12.573367  0.767308      8.387055  32.8  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ca62f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.7963279504595684 std: 0.04787713416145057 thresh: 0.7484508162981178\n",
      "Best average fit: split_0  dof              33.000000\n",
      "         accuracy          0.822050\n",
      "         cross-entropy     6.413958\n",
      "split_1  dof              33.000000\n",
      "         accuracy          0.733075\n",
      "         cross-entropy     9.620936\n",
      "split_2  dof              33.000000\n",
      "         accuracy          0.789168\n",
      "         cross-entropy     7.599145\n",
      "split_3  dof              33.000000\n",
      "         accuracy          0.841085\n",
      "         cross-entropy     5.727867\n",
      "split_4  dof              32.000000\n",
      "         accuracy          0.651163\n",
      "         cross-entropy    12.573367\n",
      "mean     accuracy          0.767308\n",
      "         cross-entropy     8.387055\n",
      "         dof              32.800000\n",
      "Name: 49, dtype: float64\n",
      "Best parameter: 3.0517578125e-05\n"
     ]
    }
   ],
   "source": [
    "sgd_best_reg = find_optimal_param(sgd_cv_res, sgd_reg_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab747a6",
   "metadata": {},
   "source": [
    "## Compare Fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fab5ca22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77169/1363510985.py:28: UserWarning: tight_layout not applied: number of columns in subplot specifications must be multiples of one another.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "steps = list(range(len(reg_params)))\n",
    "legend = [\"glmnet\", \"sklinreg\", \"sgd\"]\n",
    "\n",
    "p0 = fig.add_subplot(121)\n",
    "#p0.plot(steps, glmnet_cv_res[(\"mean\", \"dof\")], label=legend[0])\n",
    "p0.plot(steps, sk_linreg_res[(\"mean\", \"dof\")], label=legend[1])\n",
    "p0.plot(steps, sgd_cv_res[(\"mean\", \"dof\")], label=legend[2])\n",
    "p0.set_ylabel(\"DoF\")\n",
    "p0.legend()\n",
    "\n",
    "p1 = fig.add_subplot(132)\n",
    "#p1.plot(steps, glmnet_cv_res[(\"mean\", \"accuracy\")], label=legend[0])\n",
    "p1.plot(steps, sk_linreg_res[(\"mean\", \"accuracy\")], label=legend[1])\n",
    "p1.plot(steps, sgd_cv_res[(\"mean\", \"accuracy\")], label=legend[2])\n",
    "p1.set_ylabel(\"Accuracy\")\n",
    "p1.legend()\n",
    "\n",
    "p2 = fig.add_subplot(133)\n",
    "#p2.plot(steps, glmnet_reg_params, label=legend[0])\n",
    "p2.plot(steps, sk_reg_params, label=legend[1])\n",
    "p2.plot(steps, sgd_reg_params, label=legend[2])\n",
    "p2.set_yscale(\"log\")\n",
    "p2.set_ylabel(\"Regularization\")\n",
    "p2.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509ac31",
   "metadata": {},
   "source": [
    "## Run for 100 different Seeds\n",
    "\n",
    "- compare parameters to R implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d0a314f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349874"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
