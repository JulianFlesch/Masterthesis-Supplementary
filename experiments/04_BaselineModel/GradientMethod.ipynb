{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceee61af",
   "metadata": {},
   "source": [
    "This is based on Fabian Pedregosas [blog](https://fa.bianp.net/blog/2013/logistic-ordinal-regression/) and deprecated [github](https://github.com/fabianp/minirank/blob/master/minirank/logistic.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c954aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import optimize, linalg, sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ead39c",
   "metadata": {},
   "source": [
    "# Simulation Data\n",
    "\n",
    "Use a subsample of the simulation data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238cdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masterthesis.data import load_simdata, load_acinar\n",
    "data = load_simdata(\"/home/julian/Uni/MasterThesis/data/simdata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3914f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.var['Setting'] == \"TS\"  # extract only the time series samples\n",
    "sim_X = data.X[:, idx]\n",
    "sim_y = data.obs[\"Ordinal_Time_Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random subsample genes\n",
    "y_idx = rng.choice(np.arange(sim_y.size), size=sim_y.size // 2, replace=False)\n",
    "x_idx = rng.choice(np.arange(sim_X.shape[1]), size=sim_X.shape[1] // 4, replace=False)\n",
    "sim_X = sim_X[y_idx, :]\n",
    "sim_X = sim_X[:, x_idx]\n",
    "sim_y = sim_y[y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1769690",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "sim_X = scaler.fit_transform(sim_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ecaea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 1031)\n",
      "(286,)\n"
     ]
    }
   ],
   "source": [
    "print(sim_X.shape)\n",
    "print(sim_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a619471",
   "metadata": {},
   "source": [
    "# Acinar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cfc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acinar_ann = load_acinar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3204e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected Genes after preprocessing in R\n",
    "sel_genes = [\"REG3A\", \"AMY2A\", \"MT2A\", \"OLFM4\",\n",
    "             \"SYCN\", \"CELA2B\", \"FGL1\", \"AMY2B\",\n",
    "             \"MT1G\", \"TM4SF1\", \"CELA2A\", \"PDK4\", \n",
    "             \"TACSTD2\", \"CD44\", \"PNLIPRP2\", \"ALB\", \n",
    "             \"ERP27\", \"LDHA\", \"REG3G\", \"CTRL\", \"CLPS\",\n",
    "             \"FOS\", \"HSPA8\", \"SERPINA3\", \"CELA3B\", \"CRP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37beff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ac_y = np.array([int(x) for x in acinar_ann.obs.donor_age])\n",
    "ac_label_conv = dict(zip(np.unique(ac_y), range(len(ac_y))))\n",
    "ac_y = np.array([ac_label_conv[l] for l in ac_y])\n",
    "k = len(np.unique(ac_y))\n",
    "\n",
    "ac_X_train, ac_X_test, ac_y_train, ac_y_test = train_test_split(acinar_ann[:,sel_genes].X, ac_y, \n",
    "                                                                test_size=0.1, \n",
    "                                                                stratify=ac_y,\n",
    "                                                                random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bdcb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ac_X_train = scaler.fit_transform(ac_X_train)\n",
    "ac_X_test = scaler.fit_transform(ac_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66383904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239220/3365521759.py:1: RuntimeWarning: invalid value encountered in log\n",
      "  np.log(-3.34098243e-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(-3.34098243e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382c85d",
   "metadata": {},
   "source": [
    "## Gradient DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10614e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(beta, thresholds, X):\n",
    "    phi = 1 / (1 + np.exp(beta.T @ X - thresholds))\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6535f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularizer(beta, l):\n",
    "    return l * np.sum(np.abs(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1110e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG = 1e10\n",
    "\n",
    "# PROBLEM: Is the objective minimized or maximized?? -> Scipy MINIMIZES\n",
    "# log likelihood with regularization as objective function\n",
    "def objective(params, X, y, k, lamb=0.1):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = np.array([theta[i] for i in y])\n",
    "    thresholds_m1 = np.array([theta[max(0, i-1)] for i in y]) \n",
    "\n",
    "    # fit term\n",
    "    # PROBLEM: What if diff is negative? -> Log has issues, but value may be meaningful\n",
    "    diff = phi(beta, thresholds, X) - phi(beta, thresholds_m1, X)\n",
    "    loss = np.sum(np.log(diff, out=np.zeros_like(diff), where=(diff > 0)))\n",
    "\n",
    "    # regularization term\n",
    "    loss -= l1_regularizer(beta, lamb)\n",
    "    \n",
    "    # scipy MINIMIZES the loss\n",
    "    loss *= -1\n",
    "\n",
    "    #print(\"avg loss:\", loss.mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a2b46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_grad(params, X, y, k):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = np.array([theta[i] for i in y])\n",
    "    thresholds_m1 = np.array([theta[max(0, i-1)] for i in y])\n",
    "    \n",
    "    beta_grad = np.sum(X * (1 - phi(beta, thresholds, X) - phi(beta, thresholds_m1, X)),\n",
    "                       axis=1)\n",
    "    \n",
    "    \n",
    "    # first half of the gradient\n",
    "    e = np.identity(k)  # identity\n",
    "    e_expanded = np.concatenate([e[i] for i in y]).reshape(y.size, k).T\n",
    "    \n",
    "    #print(\"Thresholds shape\", thresholds.shape)\n",
    "    #print(\"E exp shape\", e_expanded.shape)\n",
    "    #print(\"threshold diff\", thresholds_m1 - thresholds)\n",
    "    \n",
    "    # PROBLEM: thresholds_m1 - thresholds can be 0 -> then the denominator becomes 0!\n",
    "    temp = (1 \n",
    "            - phi(beta, thresholds, X) \n",
    "            - np.exp(np.log(1 - (1 - np.exp(thresholds_m1 - thresholds)))))\n",
    "    t1 = e_expanded @ temp\n",
    "    \n",
    "    #print(\"phi:\", phi(beta, thresholds, X))\n",
    "    #print(\"exp:\", np.exp(thresholds_m1 - thresholds))\n",
    "    #print(\"t1\", t1)\n",
    "    #print(\"temp shape\", temp.shape)\n",
    "    #print(\"t1 shape\", t1.shape)\n",
    "    \n",
    "    # second half of the gradient\n",
    "    e_m1 = np.identity(k+1)[1:,:-1]  # identity with diagonal shifty up by one\n",
    "    e_m1_expanded = np.concatenate([e_m1[i] for i in y]).reshape(y.size, k).T\n",
    "    temp = (1 \n",
    "            - phi(beta, thresholds_m1, X) \n",
    "            - np.exp(np.log(1 - (1 - np.exp(-1 * (thresholds_m1 - thresholds))))))\n",
    "    \n",
    "    t2 = e_m1_expanded @ temp\n",
    "    \n",
    "    #print(\"t2\", t2)\n",
    "    #print(\"t2 shape\", t2.shape)\n",
    "    theta_grad = t1 + t2\n",
    "\n",
    "    #print(\"theta_grad\", theta_grad)\n",
    "\n",
    "    return np.concatenate([beta_grad, theta_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93da9788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1,2,3,3,0,0,0,1]\n",
    "e = np.identity(4)\n",
    "foo = np.concatenate([e[i] for i in y]).reshape(len(y), 4).T\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f6beff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(5,)\n",
      "[ 9.98806316e+00  3.49007917e+01 -5.52778637e-04  3.23067374e-01\n",
      " -3.92174067e-01  3.00048424e-01 -3.73982387e-01]\n",
      "31.892256931594275\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(10).reshape(2, 5)\n",
    "y = np.array([1,2,0,3,4])\n",
    "beta = np.arange(2)\n",
    "theta = np.array([-0.5, -0.1, 0.0123, 0.56, 1])\n",
    "k = theta.size\n",
    "params = np.concatenate([beta, theta])\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(objective_grad(params, X, y, k))\n",
    "print(objective(params, X, y, theta.size, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e416126",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.0\n",
       " hess_inv: array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])\n",
       "      jac: array([  0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,\n",
       "         0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,\n",
       "         0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. ,   0. , -24.5,\n",
       "       -73. , -64. , -49. , -38. , -38.5, -42. , -24. ])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 113\n",
       "      nit: 0\n",
       "     njev: 101\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "k = np.unique(ac_y_train).size\n",
    "params = np.zeros(ac_X_train.shape[1] + k)\n",
    "minimize(objective, params, args=(ac_X_train.T, ac_y_train, k), jac=objective_grad, method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed4dd2",
   "metadata": {},
   "source": [
    "## Use Jax autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4328c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit, vmap\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5b69608",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMBD = 0.1\n",
    "BIG = 1e10\n",
    "X = ac_X_train\n",
    "y = ac_y_train\n",
    "k = jnp.unique(ac_y_train).size\n",
    "\n",
    "def jphi(beta, thresholds):\n",
    "    phi = 1 / (1 + jnp.exp(jnp.dot(beta.T, X) - thresholds))\n",
    "    return phi\n",
    "\n",
    "def jl1_regularizer(beta):\n",
    "    return LMBD * jnp.sum(jnp.abs(beta))\n",
    "\n",
    "# PROBLEM: Is the objective minimized or maximized?? -> Scipy MINIMIZES\n",
    "# log likelihood with regularization as objective function\n",
    "def jobjective(params):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "\n",
    "    thresholds = jnp.array([theta[i] for i in y])\n",
    "    thresholds_m1 = jnp.array([theta[max(0, i-1)] for i in y]) \n",
    "\n",
    "    # fit term\n",
    "    # PROBLEM: What if diff is negative? -> Log has issues, but value may be meaningful\n",
    "    diff = jphi(beta, thresholds) - jphi(beta, thresholds_m1)\n",
    "    loss = jnp.nan_to_num(jnp.log(diff), nan=0)\n",
    "    loss = jnp.sum(loss)\n",
    "\n",
    "    # regularization term\n",
    "    loss -= jl1_regularizer(beta)\n",
    "    \n",
    "    # scipy MINIMIZES the loss\n",
    "    loss *= -1\n",
    "\n",
    "    #print(\"avg loss:\", loss.mean())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80391d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: inf\n",
       " hess_inv: array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])\n",
       "      jac: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)\n",
       "  message: 'NaN result encountered.'\n",
       "     nfev: 1\n",
       "      nit: 0\n",
       "     njev: 1\n",
       "   status: 3\n",
       "  success: False\n",
       "        x: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LMBD = 0.1\n",
    "BIG = 1e10\n",
    "X = ac_X_train.T\n",
    "y = ac_y_train\n",
    "k = jnp.unique(ac_y_train).size\n",
    "params = np.zeros(ac_X_train.shape[1] + k)\n",
    "\n",
    "objective_jaxder = grad(jobjective)\n",
    "minimize(jobjective, params, jac=objective_jaxder, method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a986a",
   "metadata": {},
   "source": [
    "## Gradient Model v3 \n",
    "\n",
    "Based on [mord by fabian](https://github.com/fabianp/mord/blob/master/mord/threshold_based.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(t):\n",
    "    # sigmoid function, 1 / (1 + exp(-t))\n",
    "    # stable computation\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss(Z):\n",
    "    # stable computation of the logistic loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = np.log(1 + np.exp(-Z[idx]))\n",
    "    out[~idx] = (-Z[~idx] + np.log(1 + np.exp(Z[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def obj_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * log_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    obj += alpha * 0.5 * (np.dot(w, w))\n",
    "    return obj\n",
    "\n",
    "\n",
    "def grad_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Gradient for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "    # Alpha[idx] *= -1\n",
    "    # W[idx.T] *= -1\n",
    "\n",
    "    Sigma = S * loss_fd.T * sigmoid(-S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        Sigma *= sample_weight\n",
    "\n",
    "    grad_w = X.T.dot(Sigma.sum(0)) + alpha * w\n",
    "\n",
    "    grad_theta = -Sigma.sum(1)\n",
    "    grad_c = L.T.dot(grad_theta)\n",
    "    return np.concatenate((grad_w, grad_c), axis=0)\n",
    "\n",
    "\n",
    "def threshold_fit(X, y, alpha, n_class, mode='AE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'SE'}\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1))\n",
    "    elif mode == '0-1':\n",
    "        loss_fd = np.diag(np.ones(n_class - 1)) + \\\n",
    "            np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = 1  # border case\n",
    "    elif mode == 'SE':\n",
    "        a = np.arange(n_class-1)\n",
    "        b = np.arange(n_class)\n",
    "        loss_fd = np.abs((a - b[:, None])**2 - (a - b[:, None]+1)**2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "        jac=grad_margin, bounds=bounds, options=options,\n",
    "        args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "        tol=tol)\n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
