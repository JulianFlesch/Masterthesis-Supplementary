{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceee61af",
   "metadata": {},
   "source": [
    "This is based on Fabian Pedregosas [blog](https://fa.bianp.net/blog/2013/logistic-ordinal-regression/) and deprecated [github](https://github.com/fabianp/minirank/blob/master/minirank/logistic.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c954aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import optimize, linalg, sparse\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e3cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ead39c",
   "metadata": {},
   "source": [
    "# Simulation Data\n",
    "\n",
    "Use a subsample of the simulation data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238cdea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from masterthesis.data import load_h5ad, load_acinar\n",
    "data = load_h5ad(\"/home/julian/Uni/MasterThesis/data/simdata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3914f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.var['Setting'] == \"TS\"  # extract only the time series samples\n",
    "sim_X = data.X[:, idx]\n",
    "sim_y = data.obs[\"Ordinal_Time_Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random subsample genes\n",
    "y_idx = rng.choice(np.arange(sim_y.size), size=sim_y.size // 2, replace=False)\n",
    "x_idx = rng.choice(np.arange(sim_X.shape[1]), size=sim_X.shape[1] // 4, replace=False)\n",
    "sim_X = sim_X[y_idx, :]\n",
    "sim_X = sim_X[:, x_idx]\n",
    "sim_y = sim_y[y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1769690",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "sim_X = scaler.fit_transform(sim_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ecaea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 1031)\n",
      "(286,)\n"
     ]
    }
   ],
   "source": [
    "print(sim_X.shape)\n",
    "print(sim_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a619471",
   "metadata": {},
   "source": [
    "# Acinar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02cfc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acinar_ann = load_h5ad(\"/home/julian/Uni/MasterThesis/data/acinar_sce.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3204e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected Genes after preprocessing in R\n",
    "sel_genes = [\"REG3A\", \"AMY2A\", \"MT2A\", \"OLFM4\",\n",
    "             \"SYCN\", \"CELA2B\", \"FGL1\", \"AMY2B\",\n",
    "             \"MT1G\", \"TM4SF1\", \"CELA2A\", \"PDK4\", \n",
    "             \"TACSTD2\", \"CD44\", \"PNLIPRP2\", \"ALB\", \n",
    "             \"ERP27\", \"LDHA\", \"REG3G\", \"CTRL\", \"CLPS\",\n",
    "             \"FOS\", \"HSPA8\", \"SERPINA3\", \"CELA3B\", \"CRP\"]\n",
    "sel_genes = sorted(sel_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37beff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ac_y = np.array([int(x) for x in acinar_ann.obs.donor_age])\n",
    "ac_label_conv = dict(zip(np.unique(ac_y), range(len(ac_y))))\n",
    "ac_y = np.array([ac_label_conv[l] for l in ac_y])\n",
    "k = len(np.unique(ac_y))\n",
    "\n",
    "ac_X_train, ac_X_test, ac_y_train, ac_y_test = train_test_split(acinar_ann[:,sel_genes].X, ac_y, \n",
    "                                                                test_size=0.1, \n",
    "                                                                stratify=ac_y,\n",
    "                                                                random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bce8285b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = np.sort(np.unique(ac_y))\n",
    "conversion = dict(zip(labels, np.arange(len(labels)))) \n",
    "ac_y_train_trans = np.array([conversion[e] for e in ac_y_train])\n",
    "ac_y_test_trans = np.array([conversion[e] for e in ac_y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9bdcb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ac_X_train = scaler.fit_transform(ac_X_train)\n",
    "ac_X_test = scaler.fit_transform(ac_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382c85d",
   "metadata": {},
   "source": [
    "## Gradient DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "65fdd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mord solution with loss changed to L1\n",
    "# https://github.com/fabianp/mord/blob/master/mord/threshold_based.py\n",
    "import numpy as np\n",
    "from sklearn import base, metrics\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from scipy import optimize\n",
    "\n",
    "def sigmoid(t):\n",
    "    # sigmoid function, 1 / (1 + exp(-t))\n",
    "    # stable computation\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss(Z):\n",
    "    # stable computation of the logistic loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = np.log(1 + np.exp(-Z[idx]))\n",
    "    out[~idx] = (-Z[~idx] + np.log(1 + np.exp(Z[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def obj_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * log_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    \n",
    "    # changed to L1 regularization\n",
    "    obj += alpha * np.sum(np.abs(w))\n",
    "    return obj\n",
    "\n",
    "\n",
    "def grad_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Gradient for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "    # Alpha[idx] *= -1\n",
    "    # W[idx.T] *= -1\n",
    "\n",
    "    Sigma = S * loss_fd.T * sigmoid(-S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        Sigma *= sample_weight\n",
    "\n",
    "    grad_w = X.T.dot(Sigma.sum(0)) + alpha * np.sign(w)\n",
    "\n",
    "    grad_theta = -Sigma.sum(1)\n",
    "    grad_c = L.T.dot(grad_theta)\n",
    "    return np.concatenate((grad_w, grad_c), axis=0)\n",
    "\n",
    "\n",
    "def threshold_fit(X, y, alpha, n_class, mode='AE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'SE'}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1))\n",
    "    elif mode == '0-1':\n",
    "        loss_fd = np.diag(np.ones(n_class - 1)) + \\\n",
    "            np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = 1  # border case\n",
    "    elif mode == 'SE':\n",
    "        a = np.arange(n_class-1)\n",
    "        b = np.arange(n_class)\n",
    "        loss_fd = np.abs((a - b[:, None])**2 - (a - b[:, None]+1)**2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "    \n",
    "    #sol_method = 'L-BFGS-B'\n",
    "    #sol_method = 'Newton-CG'\n",
    "    sol_method = 'SLSQP'\n",
    "    sol = optimize.minimize(obj_margin, x0, method=sol_method,\n",
    "        jac=grad_margin, bounds=bounds, options=options,\n",
    "        args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "        tol=tol)\n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def threshold_predict(X, w, theta):\n",
    "    \"\"\"\n",
    "    Class numbers are assumed to be between 0 and k-1\n",
    "    \"\"\"\n",
    "    tmp = theta[:, None] - np.asarray(X.dot(w))\n",
    "    pred = np.sum(tmp < 0, axis=0).astype(int)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def threshold_proba(X, w, theta):\n",
    "    \"\"\"\n",
    "    Class numbers are assumed to be between 0 and k-1. Assumes\n",
    "    the `sigmoid` link function is used.\n",
    "    \"\"\"\n",
    "    eta = theta[:, None] - np.asarray(X.dot(w), dtype=np.float64)\n",
    "    prob = np.pad(\n",
    "        sigmoid(eta).T,\n",
    "        pad_width=((0, 0), (1, 1)),\n",
    "        mode='constant',\n",
    "        constant_values=(0, 1))\n",
    "    return np.diff(prob)\n",
    "\n",
    "\n",
    "\n",
    "class LogisticIT(base.BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classifier that implements the ordinal logistic model\n",
    "    (Immediate-Threshold variant).\n",
    "\n",
    "    Contrary to the OrdinalLogistic model, this variant\n",
    "    minimizes a convex surrogate of the 0-1 loss, hence\n",
    "    the score associated with this object is the accuracy\n",
    "    score, i.e. the same score used in multiclass\n",
    "    classification methods (sklearn.metrics.accuracy_score).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float\n",
    "        Regularization parameter. Zero is no regularization, higher values\n",
    "        increate the squared l2 regularization.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    J. D. M. Rennie and N. Srebro, \"Loss Functions for Preference Levels :\n",
    "    Regression with Discrete Ordered Labels,\" in Proceedings of the IJCAI\n",
    "    Multidisciplinary Workshop on Advances in Preference Handling, 2005.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1., verbose=0, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        _y = np.array(y).astype(int)\n",
    "        if np.abs(_y - y).sum() > 0.1:\n",
    "            raise ValueError('y must only contain integer values')\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_class_ = self.classes_.max() - self.classes_.min() + 1\n",
    "        y_tmp = y - y.min()  # we need classes that start at zero\n",
    "        self.coef_, self.theta_ = threshold_fit(\n",
    "            X, y_tmp, self.alpha, self.n_class_,\n",
    "            mode='0-1', verbose=self.verbose, max_iter=self.max_iter,\n",
    "            sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return threshold_predict(X, self.coef_, self.theta_) +\\\n",
    "         self.classes_.min()\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return threshold_proba(X, self.coef_, self.theta_)\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        pred = self.predict(X)\n",
    "        return metrics.accuracy_score(\n",
    "            pred,\n",
    "            y,\n",
    "            sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "92b1925c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = LogisticIT(verbose=0, alpha=10)\n",
    "regressor.fit(ac_X_train, ac_y_train_trans)\n",
    "regressor.score(ac_X_test, ac_y_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e9ca3ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26988636363636365\n",
      "[ 1.23778430e-01  2.08324304e-01  1.76691039e-01  2.98701886e-01\n",
      "  5.02328584e-10 -8.90480109e-02  1.45208842e-06 -1.90163349e-01\n",
      "  5.31889679e-06 -1.13353012e-01 -1.58525750e-01  7.85131379e-04\n",
      " -1.28584612e-01 -3.67297772e-02  4.25454457e-06  1.40584411e-05\n",
      "  2.18059561e-01 -8.60156434e-06  2.75518533e-05 -1.21339646e-05\n",
      "  3.14973061e-01 -1.85079684e-01  2.32742630e-01  5.45999271e-06\n",
      "  1.36101249e-05  1.24626790e-01]\n",
      "[-1.76577103  0.5383947   0.5383947   0.59014072  0.59014072  1.20320139\n",
      "  1.20320139]\n"
     ]
    }
   ],
   "source": [
    "#regressor.coef_[regressor.coef_ < 0.0001] = 0\n",
    "print(metrics.balanced_accuracy_score(ac_y_test_trans, regressor.predict(ac_X_test)))\n",
    "print(regressor.coef_)\n",
    "print(regressor.theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49793e47",
   "metadata": {},
   "source": [
    "Gradient based on derivatives and Fabianp's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1110e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG = 1e10\n",
    "SMALL = 1e-6\n",
    "\n",
    "def phi(beta, thresholds, X):\n",
    "    phi = 1 / (1 + np.exp(beta.T @ X - thresholds))\n",
    "    return phi\n",
    "\n",
    "def l1_regularizer(beta, l):\n",
    "    return l * np.sum(np.abs(beta))\n",
    "\n",
    "# PROBLEM: Is the objective minimized or maximized?? -> Scipy MINIMIZES\n",
    "# PROBLEM: What if diff is negative? -> Log has issues, but value may be meaningful\n",
    "# PROBLEM 2: What if diff is too small? -> Log will explode\n",
    "# PROBLEM 3: Loss of precision?\n",
    "# log likelihood with regularization as objective function\n",
    "def objective(params, X, y, k, lamb=0.1, scale_by_y=True):\n",
    "    \n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = np.array([theta[i] for i in y])\n",
    "    thresholds_m1 = np.array([theta[max(0, i-1)] for i in y]) \n",
    "\n",
    "    # fit term\n",
    "    idx = (thresholds > 0)\n",
    "    diff = np.zeros_like(thresholds)\n",
    "    phi_i = phi(beta, thresholds, X)\n",
    "    phi_im1 = phi(beta, thresholds_m1, X)\n",
    "    \n",
    "    diff[idx] = (phi_i - phi_im1)[idx]\n",
    "    diff[~idx] = phi_i[~idx]\n",
    "\n",
    "    # cut off the difference at a minimum to avoid PROBLEM 2\n",
    "    #if diff.min() < SMALL:\n",
    "    #    diff = np.maximum(diff, SMALL)\n",
    "    \n",
    "    loglik = np.log(diff, out=np.zeros_like(diff), where=diff>0)\n",
    "    \n",
    "    loglik = np.sum(loglik)\n",
    "\n",
    "    # regularization term\n",
    "    loglik -= l1_regularizer(beta, lamb)\n",
    "    \n",
    "    # scale the loss with the inverse number of samples to handle PROBLEM 3\n",
    "    if scale_by_y:\n",
    "        loglik *= (1 / y.size)\n",
    "\n",
    "    return loglik\n",
    "\n",
    "def loss(*args, **kwargs):\n",
    "    return - objective(*args, **kwargs)\n",
    "\n",
    "def objective_grad(params, X, y, k, lamb=0.1, scale_by_y=True):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = np.array([theta[i] for i in y])\n",
    "    thresholds_m1 = np.array([theta[max(0, i-1)] for i in y])\n",
    "\n",
    "    # PROBLEM: thresholds_m1 - thresholds can be 0 -> then the denominator becomes 0!\n",
    "    idx = (thresholds > 0)\n",
    "    diff = np.zeros_like(thresholds)\n",
    "    phi_i = phi(beta, thresholds, X)\n",
    "    phi_im1 = phi(beta, thresholds_m1, X)\n",
    "    \n",
    "    diff[idx] = (phi_i - phi_im1)[idx]\n",
    "    diff[~idx] = phi_i[~idx]\n",
    "    \n",
    "    # BETA UPDATE\n",
    "    beta_grad = np.sum(X * (1 - diff), axis=1)\n",
    "    \n",
    "    # derivative of regularizer over beta scaled\n",
    "    beta_grad += np.sum(np.sign(beta) * (beta == 0)) * lamb\n",
    "    \n",
    "    # THETA UPDATE\n",
    "    # first half of the gradient\n",
    "    e = np.identity(k)\n",
    "    e_expanded = np.concatenate([e[i] for i in y]).reshape(y.size, k).T\n",
    "    \n",
    "    temp = (1 \n",
    "            - phi(beta, thresholds, X) \n",
    "            - np.exp(np.log(1 - (1 - np.exp(diff)))))\n",
    "    t1 = e_expanded @ temp\n",
    "    \n",
    "    # second half of the gradient\n",
    "    e_m1 = np.identity(k+1)[1:,:-1]  # identity with diagonal shifty up by one\n",
    "    e_m1_expanded = np.concatenate([e_m1[i] for i in y]).reshape(y.size, k).T\n",
    "    temp = (1 \n",
    "            - phi(beta, thresholds_m1, X) \n",
    "            - np.exp(np.log(1 - (1 - np.exp(-1 * (diff))))))\n",
    "    \n",
    "    t2 = e_m1_expanded @ temp\n",
    "    \n",
    "    theta_grad = t1 + t2\n",
    "    \n",
    "    # scaling\n",
    "    if scale_by_y:\n",
    "        theta_grad = theta_grad * (1/theta.size)\n",
    "        beta_grad = beta_grad * (1/beta.size)\n",
    "\n",
    "    return np.concatenate([beta_grad, theta_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3f0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def train(obj, grad, X, y, lamb=0.9, scaling=False, method=\"BFGS\"):\n",
    "    # flip such that X -> (genes, cells)\n",
    "    if X.shape[0] == y.size:\n",
    "        X = X.T\n",
    "    \n",
    "    n_classes = np.unique(y).size\n",
    "    params = np.zeros(X.shape[0] + n_classes)\n",
    "    \n",
    "    m = minimize(obj, params_nm, args=(X, y, n_classes, lamb, scaling), jac=grad, method=method)\n",
    "    print(m)\n",
    "    print(m.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3893ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params_nm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mac_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mac_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfgs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(obj, grad, X, y, lamb, scaling, method)\u001b[0m\n\u001b[1;32m      8\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m      9\u001b[0m params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m n_classes)\n\u001b[0;32m---> 11\u001b[0m m \u001b[38;5;241m=\u001b[39m minimize(obj, \u001b[43mparams_nm\u001b[49m, args\u001b[38;5;241m=\u001b[39m(X, y, n_classes, lamb, scaling), jac\u001b[38;5;241m=\u001b[39mgrad, method\u001b[38;5;241m=\u001b[39mmethod)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(m\u001b[38;5;241m.\u001b[39mx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_nm' is not defined"
     ]
    }
   ],
   "source": [
    "train(loss, objective_grad, ac_X_train, ac_y_train, lamb=0., scaling=True, method=\"bfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e416126",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "scale_by_y = False\n",
    "lamb = 0.9\n",
    "n_classes = np.unique(ac_y_train).size\n",
    "params = np.zeros(ac_X_train.shape[1] + n_classes)\n",
    "\n",
    "# random params\n",
    "params_rand = rng.normal(0, 0.1, ac_X_train.shape[1] + n_classes)\n",
    "\n",
    "# params after one run of Nelder-Mead \n",
    "params_nm = [2.99552789e-17, -9.12238797e-17,  8.43373503e-17,  6.28923016e-17,\n",
    "        1.29517064e-16,  2.35470728e-16,  4.64152560e-18, -2.37379072e-17,\n",
    "        9.33068062e-17, -1.31213310e-16,  8.47381150e-17, -9.14277033e-18,\n",
    "       -1.10647317e-16, -5.55542955e-17,  4.15674610e-17, -7.28795221e-17,\n",
    "        1.25000000e-04, -3.91915815e-17,  1.01399459e-16,  2.16536932e-16,\n",
    "        6.13107712e-15,  1.23078388e-16,  1.16260966e-16,  1.76600273e-17,\n",
    "        1.35598036e-16,  7.26746491e-17,  1.25000000e-04,  1.30296841e-16,\n",
    "        1.45441669e-16, -1.72678216e-17, -1.87164751e-16, -4.54465550e-17,\n",
    "       -1.00882237e-16, -3.29592381e-17]\n",
    "\n",
    "params_warm =  [0.47077975,  0.        ,  0.37275445,  0.        ,  0.        ,\n",
    "        -0.2718176 ,  0.01834662,  0.27956828,  0.        ,  0.00906079,\n",
    "         0.        ,  0.22041753, -0.1378634 ,  0.18726233, -0.08640451,\n",
    "         0.08268969, -0.26427331,  0.        , -0.28873376, -0.24587408,\n",
    "         0.        , -0.07656282, -0.02134246,  0.59662556,  0.05288458,\n",
    "         0.        ,  1.71402649,  0.10353958,  0.        , \n",
    "        -0.48783945, -0.62012817, -1.75287372, -2.09708938, 4]\n",
    "\n",
    "m = minimize(objective, params_nm, args=(ac_X_train.T, ac_y_train, n_classes, lamb, scale_by_y),\n",
    "         method=\"BFGS\")\n",
    "print(m)\n",
    "print(m.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c9eb8",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- Numerical instabilities (nans, infs): Especially BFGS struggles here and usually only executes one iteration\n",
    "- BFTS struggles to converge, starting from all-0 parameters.\n",
    "- Nelder-Mead works without derivative\n",
    "\n",
    "- None of the attempts introduce sparsity!\n",
    "- Using pretrained parameters (params_warm), all attempted solvers converge successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09674e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(obj, grad, X, y, \n",
    "              tol = 1e-8, \n",
    "              max_iter = 10000,\n",
    "              exp_dec1 = 0.9,\n",
    "              exp_dec2 = 0.999,\n",
    "              eta = 0.01,\n",
    "              epsilon = 1e-8,\n",
    "              scaling = True,\n",
    "              lamb = 0.9):\n",
    "\n",
    "    # weights and data params\n",
    "    n_classes = np.unique(y).size\n",
    "    params = np.zeros(X.shape[0] + n_classes)\n",
    "    \n",
    "    # adam\n",
    "    m_dw, v_dw = 0, 0\n",
    "    m_db, v_db = 0, 0\n",
    "\n",
    "    # iteration tracking\n",
    "    dloss = 1\n",
    "    cur_iter = 1\n",
    "    losses = []\n",
    "    while True:\n",
    "        if (dloss < tol):\n",
    "            print(\"Converged\")\n",
    "            break\n",
    "        if (cur_iter > max_iter):\n",
    "            print(\"Max iterations\")\n",
    "            break\n",
    "\n",
    "        loss = - obj(params, X, y, n_classes, lamb, scaling)\n",
    "        g = grad(params, X, y, n_classes, lamb, scaling)\n",
    "\n",
    "        # split into w, b\n",
    "        #w = params[:-n_classes]\n",
    "        #b = params[-n_classes:]\n",
    "        #dw = g[:-n_classes]\n",
    "        #db = g[-n_classes:]\n",
    "        \n",
    "        # ADAM:\n",
    "        # momentum\n",
    "        m_dw = exp_dec1 * m_dw + (1 - exp_dec1) * g\n",
    "        #m_db = exp_dec1 * m_db + (1 - exp_dec1) * db\n",
    "        # rms\n",
    "        v_dw = exp_dec2 * v_dw + (1 - exp_dec2) * (g**2)\n",
    "        #v_db = exp_dec2 * v_db + (1 - exp_dec2) * (db)\n",
    "        # bias correction\n",
    "        m_dw_corr = m_dw / (1 - exp_dec1**cur_iter)\n",
    "        #m_db_corr = m_db / (1 - exp_dec1**cur_iter)\n",
    "        v_dw_corr = v_dw / (1 - exp_dec2**cur_iter)\n",
    "        #v_db_corr = v_db / (1 - exp_dec2**cur_iter)\n",
    "\n",
    "        # update weights\n",
    "        params = params - eta * (m_dw_corr / (np.sqrt(v_dw_corr) + epsilon))\n",
    "        #params[:-n_classes] = w - eta * (m_dw_corr / (np.sqrt(v_dw_corr) + epsilon))\n",
    "        #params[-n_classes:] = b - eta * (m_db_corr / (np.sqrt(v_db_corr) + epsilon))\n",
    "        \n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1:\n",
    "            dloss = np.abs(losses[-2] - losses[-1])\n",
    "        else:\n",
    "            dloss = tol + 1\n",
    "        \n",
    "        cur_iter += 1\n",
    "\n",
    "    print(\"iterations\", cur_iter)\n",
    "    print(\"Final loss\", loss)\n",
    "    print(\"Final grad\", g)\n",
    "    print(\"Final weights\", params)\n",
    "    \n",
    "\n",
    "    return (w, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, losses = train_sgd(objective, objective_grad, ac_X_train.T, ac_y_train, scaling=True, max_iter=10000, lamb=10, eta=0.1, tol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba30833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), [-1* l for l in losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2325db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_sgd(objective, objective_grad, ac_X_train.T, ac_y_train, scaling=True, lamb=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3287f0",
   "metadata": {},
   "source": [
    "## Use Jax autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit, vmap\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05155574",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMBD = 0.5\n",
    "BIG = 1e10\n",
    "X = ac_X_train\n",
    "y = ac_y_train\n",
    "k = jnp.unique(ac_y_train).size\n",
    "\n",
    "def jphi(beta, thresholds):\n",
    "    phi = 1 / (1 + jnp.exp(jnp.dot(beta.T, X) - thresholds))\n",
    "    return phi\n",
    "\n",
    "def jl1_regularizer(beta):\n",
    "    return LMBD * jnp.sum(jnp.abs(beta))\n",
    "\n",
    "# PROBLEM: Is the objective minimized or maximized?? -> Scipy MINIMIZES\n",
    "# log likelihood with regularization as objective function\n",
    "def jobjective(params):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    thresholds = jnp.array([theta[i] for i in y])\n",
    "    thresholds_m1 = jnp.array([theta[max(0, i-1)] for i in y]) \n",
    "\n",
    "    # fit term\n",
    "    # PROBLEM: What if diff is negative? -> Log has issues, but value may be meaningful\n",
    "    idx = (thresholds != 0)\n",
    "    diff = jnp.zeros_like(thresholds)\n",
    "    phi_i = jphi(beta, thresholds)\n",
    "    phi_im1 = jphi(beta, thresholds_m1)\n",
    "    diff.at[idx].set((phi_i - phi_im1)[idx])\n",
    "    diff.at[~idx].set(phi_i[~idx])\n",
    "\n",
    "    loss = jnp.log(diff)\n",
    "    loss = jnp.nan_to_num(jnp.log(diff), nan=0)\n",
    "    loss = jnp.sum(loss)\n",
    "\n",
    "    # regularization term\n",
    "    loss -= jl1_regularizer(beta)\n",
    "    \n",
    "    # scipy MINIMIZES the loss\n",
    "    loss *= -1\n",
    "\n",
    "    #print(\"avg loss:\", loss.mean())\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333efad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "LMBD = 0.01\n",
    "BIG = 1e10\n",
    "X = ac_X_train.T\n",
    "y = ac_y_train\n",
    "k = jnp.unique(ac_y_train).size\n",
    "params = np.zeros(ac_X_train.shape[1] + k)\n",
    "\n",
    "params_ones = np.ones(ac_X_train.shape[1] + k)\n",
    "\n",
    "params_warm_start = [0.47077975,  0.        ,  0.37275445,  0.        ,  0.        ,\n",
    "        -0.2718176 ,  0.01834662,  0.27956828,  0.        ,  0.00906079,\n",
    "         0.        ,  0.22041753, -0.1378634 ,  0.18726233, -0.08640451,\n",
    "         0.08268969, -0.26427331,  0.        , -0.28873376, -0.24587408,\n",
    "         0.        , -0.07656282, -0.02134246,  0.59662556,  0.05288458,\n",
    "         0.        ,  1.71402649,  0.10353958,  0.        , 0,  -0.48783945,\n",
    "        -0.62012817, -1.75287372, -2.09708938]\n",
    "\n",
    "\n",
    "\n",
    "objective_jaxder = grad(jobjective)\n",
    "minimize(jobjective, params, jac=objective_jaxder, method=\"BFGS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
