{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceee61af",
   "metadata": {},
   "source": [
    "This is based on Fabian Pedregosas [blog](https://fa.bianp.net/blog/2013/logistic-ordinal-regression/) and deprecated [github](https://github.com/fabianp/minirank/blob/master/minirank/logistic.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c954aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import optimize, linalg, sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ead39c",
   "metadata": {},
   "source": [
    "# Simulation Data\n",
    "\n",
    "Use a subsample of the simulation data for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238cdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masterthesis.data import load_simdata, load_acinar\n",
    "data = load_simdata(\"/home/julian/Uni/MasterThesis/data/simdata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3914f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = data.var['Setting'] == \"TS\"  # extract only the time series samples\n",
    "sim_X = data.X[:, idx]\n",
    "sim_y = data.obs[\"Ordinal_Time_Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b787a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random subsample genes\n",
    "y_idx = rng.choice(np.arange(sim_y.size), size=sim_y.size // 2, replace=False)\n",
    "x_idx = rng.choice(np.arange(sim_X.shape[1]), size=sim_X.shape[1] // 4, replace=False)\n",
    "sim_X = sim_X[y_idx, :]\n",
    "sim_X = sim_X[:, x_idx]\n",
    "sim_y = sim_y[y_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1769690",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "sim_X = scaler.fit_transform(sim_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ecaea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 1031)\n",
      "(286,)\n"
     ]
    }
   ],
   "source": [
    "print(sim_X.shape)\n",
    "print(sim_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a619471",
   "metadata": {},
   "source": [
    "# Acinar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cfc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acinar_ann = load_acinar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3204e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected Genes after preprocessing in R\n",
    "sel_genes = [\"REG3A\", \"AMY2A\", \"MT2A\", \"OLFM4\",\n",
    "             \"SYCN\", \"CELA2B\", \"FGL1\", \"AMY2B\",\n",
    "             \"MT1G\", \"TM4SF1\", \"CELA2A\", \"PDK4\", \n",
    "             \"TACSTD2\", \"CD44\", \"PNLIPRP2\", \"ALB\", \n",
    "             \"ERP27\", \"LDHA\", \"REG3G\", \"CTRL\", \"CLPS\",\n",
    "             \"FOS\", \"HSPA8\", \"SERPINA3\", \"CELA3B\", \"CRP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37beff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ac_y = np.array([int(x) for x in acinar_ann.obs.donor_age])\n",
    "ac_label_conv = dict(zip(np.unique(ac_y), range(len(ac_y))))\n",
    "ac_y = np.array([ac_label_conv[l] for l in ac_y])\n",
    "k = len(np.unique(ac_y))\n",
    "\n",
    "ac_X_train, ac_X_test, ac_y_train, ac_y_test = train_test_split(acinar_ann[:,sel_genes].X, ac_y, \n",
    "                                                                test_size=0.1, \n",
    "                                                                stratify=ac_y,\n",
    "                                                                random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bdcb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ac_X_train = scaler.fit_transform(ac_X_train)\n",
    "ac_X_test = scaler.fit_transform(ac_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447c410",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Define the model based on its Loss function, the gradient and the hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e6a6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG = 1e10\n",
    "SMALL = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c262218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(t):\n",
    "    \"\"\"\n",
    "    logistic function, returns 1 / (1 + exp(-t))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size, dtype=np.float)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def log_logistic(t):\n",
    "    \"\"\"\n",
    "    (minus) logistic loss function, returns log(1 / (1 + exp(-t)))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = np.log(1 + np.exp(-t[idx]))\n",
    "    out[~idx] = (-t[~idx] + np.log(1 + np.exp(t[~idx])))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "433217e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGED FROM THE ORIGINAL! (See FABIANS Github)\n",
    "def f_obj(x0, X, y):\n",
    "        \"\"\"\n",
    "        Objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        if np.any(c > 0):\n",
    "            return BIG\n",
    "\n",
    "        loss = -np.log(1 - np.exp(c)).sum()\n",
    "\n",
    "        loss += b.sum() + log_logistic(b).sum() \\\n",
    "            + log_logistic(a).sum() \\\n",
    "            + alpha * w.dot(w) - np.log(z).sum() # penalty\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f2949a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_grad(x0, X, y):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        # gradient for w\n",
    "        phi_a = phi(a)\n",
    "        phi_b = phi(b)\n",
    "        grad_w = -X[k1:].T.dot(phi_b) + X.T.dot(1 - phi_a) + alpha * w\n",
    "\n",
    "        # gradient for theta\n",
    "        idx = c > 0\n",
    "        tmp = np.empty_like(c)\n",
    "        tmp[idx] = 1. / (np.exp(-c[idx]) - 1)\n",
    "        tmp[~idx] = np.exp(c[~idx]) / (1 - np.exp(c[~idx])) # should not need\n",
    "        grad_theta = (E1 - E0)[:, k1:].dot(tmp) \\\n",
    "            + E0[:, k1:].dot(phi_b) - E0.dot(1 - phi_a)\n",
    "\n",
    "        grad_theta[:-1] += 1. / np.diff(theta_0)\n",
    "        grad_theta[1:] -= 1. / np.diff(theta_0)\n",
    "        out = np.concatenate((grad_w, grad_theta))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78e36597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_hess(x0, s, X, y):\n",
    "        x0 = np.asarray(x0)\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        D = np.diag(phi(a) * (1 - phi(a)))\n",
    "        D_= np.diag(phi(b) * (1 - phi(b)))\n",
    "        D1 = np.diag(np.exp(-c) / (np.exp(-c) - 1) ** 2)\n",
    "        Ex = (E1 - E0)[:, k1:].toarray()\n",
    "        Ex0 = E0.toarray()\n",
    "        H_A = X[k1:].T.dot(D_).dot(X[k1:]) + X.T.dot(D).dot(X)\n",
    "        H_C = - X[k1:].T.dot(D_).dot(E0[:, k1:].T.toarray()) \\\n",
    "            - X.T.dot(D).dot(E0.T.toarray())\n",
    "        H_B = Ex.dot(D1).dot(Ex.T) + Ex0[:, k1:].dot(D_).dot(Ex0[:, k1:].T) \\\n",
    "            - Ex0.dot(D).dot(Ex0.T)\n",
    "\n",
    "        p_w = H_A.shape[0]\n",
    "        tmp0 = H_A.dot(s[:p_w]) + H_C.dot(s[p_w:])\n",
    "        tmp1 = H_C.T.dot(s[:p_w]) + H_B.dot(s[p_w:])\n",
    "        return np.concatenate((tmp0, tmp1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd59c9",
   "metadata": {},
   "source": [
    "# Scipy Optimize fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9c6e0e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167575/2606101717.py:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  E0 = (y[:, np.newaxis] == unq_y).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# change according to dataset currently used\n",
    "X = ac_X_train\n",
    "y = ac_y_train\n",
    "#X = sim_X\n",
    "#y = sim_y\n",
    "\n",
    "# regularization\n",
    "alpha = 0\n",
    "\n",
    "# weird but important model stuff\n",
    "unq_y = np.unique(y)\n",
    "k1 = np.sum(y == unq_y[0])\n",
    "E0 = (y[:, np.newaxis] == unq_y).astype(np.int)\n",
    "E1 = np.roll(E0, -1, axis=-1)\n",
    "E1[:, -1] = 0.\n",
    "E0, E1 = map(sparse.csr_matrix, (E0.T, E1.T))\n",
    "\n",
    "# fiting options\n",
    "options = {'maxiter' : 1000, 'disp': 0, 'maxfun':10000}\n",
    "\n",
    "# initial weights\n",
    "x0 = rng.random(X.shape[1] + np.unique(y).size) / X.shape[1]\n",
    "\n",
    "# fit\n",
    "out = optimize.minimize(f_obj, x0, args=(X, y), method=\"Nelder-Mead\", jac=f_grad, hessp=f_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c403ead4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[0.0001633 , 0.03156496, 0.03187995, ..., 0.01427545, 0.00545713,\n",
       "        0.02498236],\n",
       "       [0.00016355, 0.03156496, 0.03187995, ..., 0.01427545, 0.00545713,\n",
       "        0.02498236],\n",
       "       [0.0001633 , 0.03161428, 0.03187995, ..., 0.01427545, 0.00545713,\n",
       "        0.02498236],\n",
       "       ...,\n",
       "       [0.0001633 , 0.03156496, 0.03187995, ..., 0.01429775, 0.00545713,\n",
       "        0.02498236],\n",
       "       [0.0001633 , 0.03156496, 0.03187995, ..., 0.01427545, 0.00546566,\n",
       "        0.02498236],\n",
       "       [0.0001633 , 0.03156496, 0.03187995, ..., 0.01427545, 0.00545713,\n",
       "        0.02502139]]), array([1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10,\n",
       "       1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10,\n",
       "       1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10,\n",
       "       1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10, 1.e+10,\n",
       "       1.e+10, 1.e+10, 1.e+10]))\n",
       "           fun: 10000000000.0\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 215\n",
       "           nit: 6\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([0.0001633 , 0.03156496, 0.03187995, 0.03091717, 0.0384549 ,\n",
       "       0.0073963 , 0.0359252 , 0.01363151, 0.02893065, 0.03354644,\n",
       "       0.02433603, 0.0368514 , 0.00468751, 0.03684127, 0.0320742 ,\n",
       "       0.01265816, 0.01388895, 0.00327318, 0.02924875, 0.03686002,\n",
       "       0.01431443, 0.02668549, 0.02105812, 0.02726477, 0.01163325,\n",
       "       0.01348207, 0.01308777, 0.0323734 , 0.0215121 , 0.02762218,\n",
       "       0.02904263, 0.01427545, 0.00545713, 0.02498236])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f7372",
   "metadata": {},
   "source": [
    "## Gradient DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2449d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(X, beta, theta):\n",
    "    return 1 / (1 + np.matmul(beta.T, X) - theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0a8c4a70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15384615, 0.14084507, 0.12519248, 0.11848341, 0.11111111])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(10).reshape(2,5)\n",
    "y = np.array([1,2,0,3,4])\n",
    "beta = np.arange(2)\n",
    "theta = np.array([-0.5, -0.1, 0.0123, 0.56, 1])\n",
    "k = theta.size\n",
    "params = np.concatenate([beta, theta])\n",
    "phi(X, beta, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9b06dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularizer(beta, l):\n",
    "    return l * np.sum(np.abs(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "071f8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG = 10000000000\n",
    "\n",
    "# log likelihood with regularization as objective function\n",
    "def objective(params, X, y, k, lamb=100):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    theta1 = np.array([theta[i] for i in y])\n",
    "    theta2 = np.array([theta[i-1] if i > 0 else -BIG for i in y]) \n",
    "    # fit term\n",
    "    likelihood = -1 * np.sum(np.log(phi(X, beta, theta1) - phi(X, beta, theta2)))\n",
    "    # regularization term\n",
    "    likelihood += l1_regularizer(beta, lamb)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cba39340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_grad(params, X, y, k):\n",
    "    beta = params[:-k]\n",
    "    theta = params[-k:]\n",
    "    theta1 = np.array([theta[i] for i in y])\n",
    "    theta2 = np.array([theta[i-1] if i > 0 else -BIG for i in y])\n",
    "    \n",
    "    beta_grad = np.sum(X * (1 - phi(X, beta, theta1) - phi(X, beta, theta2)),\n",
    "                       axis=1)\n",
    "    \n",
    "    # first half of the gradient\n",
    "    temp1 = 1 - phi(X, beta, theta) - (1 / (1 - np.exp(theta2 - theta1)))\n",
    "    temp2 = 1 - phi(X, beta, theta) - (1 / (1 - np.exp(-1 * (theta2 - theta1))))\n",
    "    theta_grad = temp1 + np.concatenate([[0], temp2[:-1]]) \n",
    "\n",
    "    return np.concatenate([beta_grad, theta_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3c7eacc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.923339   27.09248244]\n",
      "[-2.18709094 -5.67552231  9.14803831 -0.61490804  0.33236145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191274/2385301824.py:13: RuntimeWarning: overflow encountered in exp\n",
      "  temp2 = 1 - phi(X, beta, theta) - (1 / (1 - np.exp(-1 * (theta2 - theta1))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.923339  , 27.09248244, -2.18709094, -5.67552231,  9.14803831,\n",
       "       -0.61490804,  0.33236145])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_grad(params, X, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b49e7119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.12130433664053"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective(params, X, y, theta.size, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cf397c42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191274/211562064.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  temp2 = 1 - phi(X, beta, theta) - (1 / (1 - np.exp(-1 * (theta2 - theta1))))\n",
      "/tmp/ipykernel_191274/722998437.py:10: RuntimeWarning: invalid value encountered in log\n",
      "  likelihood = -1 * np.sum(np.log(phi(X, beta, theta1) - phi(X, beta, theta2)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: nan\n",
       "     jac: array([278.99167735, 815.50787302,  -5.51799945,  34.75405763,\n",
       "       -17.72646518,  79.73272709,  76.10792446])\n",
       " message: 'Iteration limit reached'\n",
       "    nfev: 1070\n",
       "     nit: 100\n",
       "    njev: 100\n",
       "  status: 9\n",
       " success: False\n",
       "       x: array([ 1.25061769, -0.29190542,  1.49156664,  1.64499377,  1.60628663,\n",
       "        2.43035065,  1.82246599])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(objective, params, args=(X,y,k), jac=objective_grad, method=\"SLSQP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309900f",
   "metadata": {},
   "source": [
    "## Gradient Model v3 \n",
    "\n",
    "Based on [mord by fabian](https://github.com/fabianp/mord/blob/master/mord/threshold_based.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb73337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(t):\n",
    "    # sigmoid function, 1 / (1 + exp(-t))\n",
    "    # stable computation\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "\n",
    "def log_loss(Z):\n",
    "    # stable computation of the logistic loss\n",
    "    idx = Z > 0\n",
    "    out = np.zeros_like(Z)\n",
    "    out[idx] = np.log(1 + np.exp(-Z[idx]))\n",
    "    out[~idx] = (-Z[~idx] + np.log(1 + np.exp(Z[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def obj_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Objective function for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "\n",
    "    err = loss_fd.T * log_loss(S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        err *= sample_weight\n",
    "    obj = np.sum(err)\n",
    "    obj += alpha * 0.5 * (np.dot(w, w))\n",
    "    return obj\n",
    "\n",
    "\n",
    "def grad_margin(x0, X, y, alpha, n_class, weights, L, sample_weight):\n",
    "    \"\"\"\n",
    "    Gradient for the general margin-based formulation\n",
    "    \"\"\"\n",
    "\n",
    "    w = x0[:X.shape[1]]\n",
    "    c = x0[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    loss_fd = weights[y]\n",
    "\n",
    "    Xw = X.dot(w)\n",
    "    Alpha = theta[:, None] - Xw  # (n_class - 1, n_samples)\n",
    "    S = np.sign(np.arange(n_class - 1)[:, None] - y + 0.5)\n",
    "    # Alpha[idx] *= -1\n",
    "    # W[idx.T] *= -1\n",
    "\n",
    "    Sigma = S * loss_fd.T * sigmoid(-S * Alpha)\n",
    "    if sample_weight is not None:\n",
    "        Sigma *= sample_weight\n",
    "\n",
    "    grad_w = X.T.dot(Sigma.sum(0)) + alpha * w\n",
    "\n",
    "    grad_theta = -Sigma.sum(1)\n",
    "    grad_c = L.T.dot(grad_theta)\n",
    "    return np.concatenate((grad_w, grad_c), axis=0)\n",
    "\n",
    "\n",
    "def threshold_fit(X, y, alpha, n_class, mode='AE',\n",
    "                  max_iter=1000, verbose=False, tol=1e-12,\n",
    "                  sample_weight=None):\n",
    "    \"\"\"\n",
    "    Solve the general threshold-based ordinal regression model\n",
    "    using the logistic loss as surrogate of the 0-1 loss\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : string, one of {'AE', '0-1', 'SE'}\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = check_X_y(X, y, accept_sparse='csr')\n",
    "    unique_y = np.sort(np.unique(y))\n",
    "    if not np.all(unique_y == np.arange(unique_y.size)):\n",
    "        raise ValueError(\n",
    "            'Values in y must be %s, instead got %s'\n",
    "            % (np.arange(unique_y.size), unique_y))\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # convert from c to theta\n",
    "    L = np.zeros((n_class - 1, n_class - 1))\n",
    "    L[np.tril_indices(n_class-1)] = 1.\n",
    "\n",
    "    if mode == 'AE':\n",
    "        # loss forward difference\n",
    "        loss_fd = np.ones((n_class, n_class - 1))\n",
    "    elif mode == '0-1':\n",
    "        loss_fd = np.diag(np.ones(n_class - 1)) + \\\n",
    "            np.diag(np.ones(n_class - 2), k=-1)\n",
    "        loss_fd = np.vstack((loss_fd, np.zeros(n_class - 1)))\n",
    "        loss_fd[-1, -1] = 1  # border case\n",
    "    elif mode == 'SE':\n",
    "        a = np.arange(n_class-1)\n",
    "        b = np.arange(n_class)\n",
    "        loss_fd = np.abs((a - b[:, None])**2 - (a - b[:, None]+1)**2)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x0 = np.zeros(n_features + n_class - 1)\n",
    "    x0[X.shape[1]:] = np.arange(n_class - 1)\n",
    "    options = {'maxiter' : max_iter, 'disp': verbose}\n",
    "    if n_class > 2:\n",
    "        bounds = [(None, None)] * (n_features + 1) + \\\n",
    "                 [(0, None)] * (n_class - 2)\n",
    "    else:\n",
    "        bounds = None\n",
    "\n",
    "    sol = optimize.minimize(obj_margin, x0, method='L-BFGS-B',\n",
    "        jac=grad_margin, bounds=bounds, options=options,\n",
    "        args=(X, y, alpha, n_class, loss_fd, L, sample_weight),\n",
    "        tol=tol)\n",
    "    if verbose and not sol.success:\n",
    "        print(sol.message)\n",
    "\n",
    "    w, c = sol.x[:X.shape[1]], sol.x[X.shape[1]:]\n",
    "    theta = L.dot(c)\n",
    "    return w, theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
