Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   285    616.3 MiB    616.3 MiB           1       @profile
   286                                             def fit(self, X, y, sample_weight=None):
   287                                         
   288    616.3 MiB      0.0 MiB           1           rng = np.random.default_rng(self.random_state)
   289    616.3 MiB      0.0 MiB           1           X, y = self._before_fit(X, y)
   290                                         
   291    616.3 MiB      0.0 MiB           1           if self.early_stopping:
   292                                                     # TODO: This is a full copy of the input data -> split an index array instead and work with slices?
   293    616.3 MiB      0.0 MiB           1               X, X_test, y, y_test = train_test_split(X, y, test_size=self.validation_fraction, stratify=y, random_state=rng.integers(9999))
   294                                                     
   295                                                     # TODO: initializing binarized matrices for testing can be significant memory sink!
   296    616.3 MiB      0.0 MiB           1               y_test_bin = restructure_y_to_bin(y_test)
   297    616.3 MiB      0.0 MiB           1               del(y_test)
   298                                         
   299    616.3 MiB      0.0 MiB           1               if self.early_stopping_batches:
   300    616.3 MiB      0.0 MiB           1                   n_test = X_test.shape[0]
   301    616.3 MiB      0.0 MiB           1                   test_indices = np.arange(len(y_test_bin))
   302                                                     else:
   303                                                         X_test_bin = restructure_X_to_bin(X_test, self.k_)
   304                                                         del(X_test)
   305                                                 
   306                                                 # diagonal matrix, to construct the binarized X per batch
   307    616.3 MiB      0.0 MiB           1           thresholds = np.identity(self.k_)
   308    616.3 MiB      0.0 MiB           1           if sparse.issparse(X):
   309                                                     thresholds = sparse.crs_matrix(thresholds)
   310                                         
   311    616.3 MiB      0.0 MiB           1           model = self.get_binary_estimator()
   312    616.3 MiB      0.0 MiB           1           n = X.shape[0]
   313                                         
   314                                                 # binarize only the labels already
   315    616.3 MiB      0.0 MiB           1           y_bin = restructure_y_to_bin(y)
   316                                                 
   317                                                 # create an inex array and shuffle
   318    616.3 MiB      0.0 MiB           1           sampled_indices = rng.integers(len(y_bin), size=len(y_bin))
   319                                         
   320                                                 # iterations over all data
   321    616.3 MiB      0.0 MiB           1           epoch = 0
   322                                         
   323                                                 # tracking previous scores for early stopping
   324    616.3 MiB      0.0 MiB           1           best_score = - np.inf
   325    616.3 MiB      0.0 MiB           1           n_no_improvement = 0
   326                                         
   327    713.1 MiB      0.0 MiB           6           while epoch < self.max_iter:
   328                                         
   329    713.1 MiB      0.0 MiB           6               epoch += 1
   330                                         
   331    713.1 MiB      0.0 MiB           6               start = 0
   332    713.1 MiB      0.0 MiB          66               for i in range(1, self.n_batches+1):
   333    713.1 MiB      0.0 MiB          60                   end = (i * len(y_bin) // self.n_batches)
   334    713.1 MiB      0.0 MiB          60                   batch_idx = sampled_indices[start:end]
   335    713.1 MiB      0.0 MiB          60                   batch_idx_mod_n = batch_idx % n
   336                                                         
   337    713.1 MiB      0.0 MiB          60                   if sparse.issparse(X):
   338                                                             X_batch = sparse.hstack((X[batch_idx_mod_n], thresholds[batch_idx // n]))
   339                                                         else:
   340    713.1 MiB     96.7 MiB          60                       X_batch = np.hstack((X[batch_idx_mod_n,:], thresholds[batch_idx // n]))
   341                                                         
   342    713.1 MiB      0.0 MiB          60                   y_batch = y_bin[batch_idx]
   343    713.1 MiB      0.0 MiB          60                   start = end
   344    713.1 MiB      0.0 MiB          60                   weights = np.array(sample_weight)[batch_idx_mod_n] if sample_weight is not None else None
   345    713.1 MiB      0.0 MiB          60                   model.partial_fit(X_batch, y_batch, classes=np.unique(y_batch), sample_weight=weights)
   346                                         
   347                                                     # Early stopping using the test data 
   348    713.1 MiB      0.0 MiB           6               if self.early_stopping:
   349                                         
   350                                                         # build test data in batches as needed to avoid keeping in memory
   351    713.1 MiB      0.0 MiB           6                   if self.early_stopping_batches:
   352    713.1 MiB      0.0 MiB           6                       scores = []
   353    713.1 MiB      0.0 MiB           6                       start = 0
   354    713.1 MiB      0.0 MiB          66                       for i in range(1, self.n_batches+1):
   355    713.1 MiB      0.0 MiB          60                           end = (i * len(y_test_bin) // self.n_batches)
   356    713.1 MiB      0.0 MiB          60                           batch_idx = test_indices[start:end]
   357    713.1 MiB      0.0 MiB          60                           batch_idx_mod_n = batch_idx % n_test
   358    713.1 MiB      0.0 MiB          60                           if sparse.issparse(X_test):
   359                                                                     X_test_batch = sparse.hstack((X_test[batch_idx_mod_n], thresholds[batch_idx // n_test]))
   360                                                                 else:
   361    713.1 MiB      0.0 MiB          60                               X_test_batch = np.hstack((X_test[batch_idx_mod_n], thresholds[batch_idx // n_test]))
   362                                                                 
   363    713.1 MiB      0.0 MiB          60                           scores.append(model.score(X_test_batch, y_test_bin[batch_idx]))
   364    713.1 MiB      0.0 MiB          60                           start = end          
   365                                                                 
   366    713.1 MiB      0.0 MiB           6                       cur_score = np.mean(scores)
   367                                                         
   368                                                         else:
   369                                                             cur_score = model.score(X_test_bin, y_test_bin)
   370                                         
   371    713.1 MiB      0.0 MiB           6                   if cur_score - self.tol > best_score:
   372    686.2 MiB      0.0 MiB           1                       best_score = cur_score
   373    686.2 MiB      0.0 MiB           1                       n_no_improvement = 0
   374                                                         else:
   375    713.1 MiB      0.0 MiB           5                       n_no_improvement += 1
   376    713.1 MiB      0.0 MiB           5                       if n_no_improvement >= self.n_iter_no_change:
   377    713.1 MiB      0.0 MiB           1                           if self.verbosity >= 2:
   378                                                                     print("Stopped early at epoch ", epoch, " Current score:", cur_score)
   379    713.1 MiB      0.0 MiB           1                           break
   380                                         
   381    713.1 MiB      0.0 MiB           5               if self.shuffle:
   382    713.1 MiB      0.0 MiB           5                   sampled_indices = rng.integers(len(y_bin), size=len(y_bin))
   383                                         
   384                                                     # TODO: Learning Rate adjustments?
   385                                         
   386    713.1 MiB      0.0 MiB           1           self._after_fit(model)
   387    713.1 MiB      0.0 MiB           1           return self


